{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "import random\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from keras.layers import Conv1D, MaxPooling1D, Bidirectional, LSTM, Dropout, Dense, Layer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dense, Flatten\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.float_format', '{:.0f}'.format)\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results(a,b,c,d,e,f,g):\n",
    "    current_time = datetime.now()\n",
    "    data = {\n",
    "        'model' : [g],\n",
    "        'sim' : [a],\n",
    "        'mae' : [b],\n",
    "        'rmse' : [c],\n",
    "        'fsd' : [d],\n",
    "        'R' : [e],\n",
    "        'NSE': [f],\n",
    "        'time' : [current_time]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    with open('results_combine.csv', 'a', newline='') as f:\n",
    "        if os.path.isfile('results_combine.csv'):\n",
    "            df.to_csv('results_combine.csv', mode='a', header=False, index=False)\n",
    "        else:\n",
    "            df.to_csv('results_combine.csv', index=False)\n",
    "\n",
    "def to_df(data_list):\n",
    "    X_df = [i[:-1] for i in data_list]\n",
    "    y_df = [i[-1] for i in data_list]\n",
    "    transposed_lists = [list(x) for x in zip(*X_df)]\n",
    "\n",
    "    df_list = pd.DataFrame({f'Column{i+1}': lst for i, lst in enumerate(transposed_lists)})\n",
    "    df_list['Target'] = y_df\n",
    "    return df_list\n",
    "\n",
    "def transform_to_multivariate(data, T):\n",
    "    M = []\n",
    "    for i in range(len(data) - T):\n",
    "        row = data[i:i + T + 1]\n",
    "        M.append(row)\n",
    "    return np.array(M)\n",
    "\n",
    "def calculate_similarity(value_lst_after, value_lst_before):\n",
    "        T = len(value_lst_after)  # Number of missing values\n",
    "        similarity_sum = 0\n",
    "\n",
    "        for i in range(T):\n",
    "            yi = value_lst_after[i]\n",
    "            xi = value_lst_before[i]\n",
    "            similarity_sum += 1 / (1 + abs(yi - xi) / (max(value_lst_before) - min(value_lst_before)))\n",
    "\n",
    "        similarity = similarity_sum / T\n",
    "        return similarity\n",
    "\n",
    "def calculate_MAE(value_lst_missing, value_lst_after):\n",
    "        return mean_absolute_error(value_lst_missing, value_lst_after)\n",
    "\n",
    "def calculate_RMSE(value_lst_missing, value_lst_after):\n",
    "    return np.sqrt(mean_squared_error(value_lst_missing, value_lst_after))\n",
    "\n",
    "def calculate_FB(value_lst_missing, value_lst_after):\n",
    "    return 2 * abs((np.mean(value_lst_after) - np.mean(value_lst_missing)) / (np.mean(value_lst_after) + np.mean(value_lst_missing)))\n",
    "\n",
    "def calculate_fsd(value_lst_missing, value_lst_after):\n",
    "    std_dev_Y = np.std(value_lst_after)\n",
    "    std_dev_X = np.std(value_lst_missing)\n",
    "\n",
    "    if std_dev_X == 0:\n",
    "        return None\n",
    "    \n",
    "    fsd = 2 * abs((std_dev_Y - std_dev_X) / (std_dev_X + std_dev_Y))\n",
    "    \n",
    "    return fsd\n",
    "\n",
    "def calculate_r_score(value_lst_missing, value_lst_after):\n",
    "\n",
    "    correlation_matrix = np.corrcoef(value_lst_missing, value_lst_after)\n",
    "    r_score = correlation_matrix[0, 1]\n",
    "    return r_score\n",
    "\n",
    "def calculate_nse(value_lst_missing, value_lst_after):\n",
    "\n",
    "    value_lst_missing = np.array(value_lst_missing)\n",
    "    value_lst_after = np.array(value_lst_after)\n",
    "\n",
    "    numerator = np.sum((value_lst_missing - value_lst_after)**2)\n",
    "    denominator = np.sum((value_lst_missing - np.mean(value_lst_missing))**2)\n",
    "\n",
    "    nse = 1 - (numerator / denominator)\n",
    "    \n",
    "    return nse\n",
    "\n",
    "def calculate_metrics_for_combine(value_lst_after,name_model):\n",
    "    \n",
    "    df_before_missing = pd.read_csv('waterlevel.csv')\n",
    "    value_lst_missing = df_before_missing['Waterlevel'].values.tolist()[nan_index:nan_index+size_of_gap]\n",
    "\n",
    "\n",
    "    similarity_score = calculate_similarity(value_lst_after, value_lst_missing)\n",
    "    MAE_score = calculate_MAE(value_lst_missing, value_lst_after)\n",
    "    RMSE_score = calculate_RMSE(value_lst_missing, value_lst_after)\n",
    "    FSD_score = calculate_fsd(value_lst_missing, value_lst_after)\n",
    "    R_score = calculate_r_score(value_lst_missing, value_lst_after)\n",
    "    NSE_score = calculate_nse(value_lst_missing, value_lst_after)\n",
    "    \n",
    "    sim_lst_combine.append(similarity_score)\n",
    "    mae_lst_combine.append(MAE_score)\n",
    "    rmse_lst_combine.append(RMSE_score)\n",
    "    fsd_lst_combine.append(FSD_score)\n",
    "    r_lst_combine.append(R_score)\n",
    "    nse_lst_combine.append(NSE_score)\n",
    "\n",
    "    \n",
    "    print('\\nOri_data:', value_lst_missing)\n",
    "    print('\\nvalue_data:', value_lst_after)\n",
    "    print('\\nSimilarity_score:', similarity_score)\n",
    "    print('\\nMean Absolute Error (MAE):', MAE_score)\n",
    "    print('\\nRoot Mean Squared Error (RMSE):', RMSE_score)\n",
    "    print('\\nFraction of Standard Deviation Score:', FSD_score)\n",
    "    print('\\nR score:', R_score)\n",
    "    print('\\nThe Nash Sutcliffe efficiency (NSE):', NSE_score)\n",
    "\n",
    "    results(similarity_score, MAE_score, RMSE_score, FSD_score, R_score, NSE_score,name_model)\n",
    "\n",
    "def create_continuous_missing_values(dataframe, column_name, num_missing_values):\n",
    "    modified_df = dataframe.copy()\n",
    "    \n",
    "    if len(dataframe) > num_missing_values:\n",
    "        random_index = random.randint(0, len(dataframe) - num_missing_values)\n",
    "        modified_df.loc[random_index:random_index + num_missing_values - 1, column_name] = np.nan\n",
    "    else:\n",
    "        print(\"Error: The number of missing values requested exceeds the DataFrame's capacity.\")\n",
    "    return modified_df\n",
    "\n",
    "\n",
    "sim_lst_combine = []\n",
    "mae_lst_combine = []\n",
    "rmse_lst_combine = []\n",
    "fsd_lst_combine = []\n",
    "r_lst_combine = []\n",
    "nse_lst_combine = []\n",
    "\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_df_path = 'waterlevel.csv'\n",
    "# try:\n",
    "#     original_df = pd.read_csv(original_df_path)\n",
    "    \n",
    "#     for i in range(0, 12):\n",
    "#         modified_df = create_continuous_missing_values(original_df, 'Waterlevel', 48)\n",
    "#         modified_df.to_csv(f'waterlevel_missing_test_{i}.csv', index=False)\n",
    "        \n",
    "#         print(f'waterlevel_missing_test_{i}.csv saved with continuous missing values.')\n",
    "\n",
    "# except FileNotFoundError:\n",
    "#     print(f\"Failed to find '{original_df_path}'. Please check the file path and try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_combine(X_train):\n",
    "    combine = tf.keras.models.Sequential()\n",
    "\n",
    "    combine.add(Conv1D(filters=128, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "    combine.add(MaxPooling1D(pool_size=2))\n",
    "    combine.add(Dropout(0.15))\n",
    "    \n",
    "    combine.add(Conv1D(filters=256, kernel_size=3, activation='relu'))\n",
    "    combine.add(Dropout(0.15))\n",
    "    \n",
    "    combine.add(Conv1D(filters=512, kernel_size=3, activation='relu'))\n",
    "    combine.add(Dropout(0.15))\n",
    "    \n",
    "    combine.add(Bidirectional(LSTM(units=64, return_sequences=True)))\n",
    "    combine.add(Dropout(0.15))\n",
    "\n",
    "    class Attention(Layer):\n",
    "        def __init__(self, **kwargs):\n",
    "            super(Attention, self).__init__(**kwargs)\n",
    "    \n",
    "        def build(self, input_shape):\n",
    "            self.W = self.add_weight(name='attention_weight', shape=(input_shape[-1], 1), \n",
    "                                     initializer='random_normal', trainable=True)\n",
    "            self.b = self.add_weight(name='attention_bias', shape=(input_shape[1], 1), \n",
    "                                     initializer='zeros', trainable=True)        \n",
    "            super(Attention, self).build(input_shape)\n",
    "     \n",
    "        def call(self, x):\n",
    "            e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "            e = K.squeeze(e, axis=-1)\n",
    "            alpha = K.softmax(e)\n",
    "            alpha = K.expand_dims(alpha, axis=-1)\n",
    "            context = x * alpha\n",
    "            context = K.sum(context, axis=1)\n",
    "            return context\n",
    "\n",
    "    combine.add(Attention())\n",
    "    \n",
    "    combine.add(Dense(units=64, activation='relu'))\n",
    "    \n",
    "    combine.add(Dense(units=1))\n",
    "    \n",
    "    combine.compile(optimizer=Adam(), loss=\"mean_squared_error\", metrics=['mae'])\n",
    "    \n",
    "    return combine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### epochs=200, batch_size=16, callbacks (patience=30), validation_split=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "# tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "weights_folder = 'model_weights'\n",
    "os.makedirs(weights_folder, exist_ok=True)\n",
    "\n",
    "for location in range(4,12):\n",
    "    \n",
    "    filename = f'waterlevel_missing/waterlevel_missing_{location}.csv'\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    #### Check size of missing value\n",
    "    size_of_gap = df['Waterlevel'].isna().sum()\n",
    "\n",
    "    data = df['Waterlevel'].values.tolist()\n",
    "\n",
    "    nan_index = None\n",
    "    for j, value in enumerate(data):\n",
    "        if value != value:  # Check if the value is NaN\n",
    "            nan_index = j\n",
    "            break\n",
    "        \n",
    "    df_check_before = data[:(3*size_of_gap)+1]\n",
    "    df_check_after = data[::-1][:3*size_of_gap+1]\n",
    "    df_miss = data[nan_index:nan_index + size_of_gap]\n",
    "\n",
    "    last_data = data[:nan_index]\n",
    "    first_data = data[nan_index+size_of_gap:][::-1]\n",
    "\n",
    "    # check if missing values is in the first 3 x T data original\n",
    "    if all(value in df_check_before for value in df_miss):\n",
    "        print('\\nAll values in df_miss is in the first !!!')\n",
    "\n",
    "        first_value_combine = transform_to_multivariate(first_data, size_of_gap)\n",
    "                \n",
    "        df_list = to_df(first_value_combine)\n",
    "        \n",
    "\n",
    "        X = np.array(df_list.iloc[:, :-1])\n",
    "        X = scaler.fit_transform(X)\n",
    "        \n",
    "        y = np.array(df_list.iloc[:, -1])\n",
    "        \n",
    "        model = model_combine(X)\n",
    "        \n",
    "        weights_path = os.path.join(weights_folder, f'model_weights_location_{location}.h5')\n",
    "        if os.path.exists(weights_path):\n",
    "            model.load_weights(weights_path)\n",
    "            print(f\"Loaded weights for location {location}\")\n",
    "        else:\n",
    "            model.fit(X, y, epochs=200, batch_size=16, callbacks=[early_stopping], validation_split=0.2)\n",
    "            model.save_weights(weights_path)\n",
    "            print(f\"Saved weights for location {location}\")\n",
    "            \n",
    "            \n",
    "        # model.fit(X, y, epochs=200, batch_size=16, callbacks=[early_stopping], validation_split=0.2)\n",
    "\n",
    "        data_test = df.values.tolist()[nan_index : nan_index + 2 * size_of_gap][::-1]\n",
    "        data_test = np.concatenate(data_test).ravel()   \n",
    "\n",
    "        results_first_combine = []\n",
    "        for i in range(len(data_test)//2):\n",
    "            data_first = data_test[i:i+1+size_of_gap]\n",
    "\n",
    "            data_first[size_of_gap] = model.predict(scaler.transform(np.array(data_first[:size_of_gap]).reshape(1,-1)))\n",
    "\n",
    "            results_first_combine.append(data_first[size_of_gap])\n",
    "            \n",
    "        ###################################################################\n",
    "        print('\\n', 'result of combine only (first):')                        #  \n",
    "        calculate_metrics_for_combine(results_first_combine,'results_combine (FIRST)')#\n",
    "        print('\\n')                                                       #\n",
    "        ###################################################################\n",
    "        \n",
    "        df = pd.read_csv('Impute_misvalues_hungyen.csv')\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(df['Waterlevel'].values.tolist()[nan_index:nan_index+size_of_gap], label='Water Level')\n",
    "        plt.plot(results_first_combine, label='Predicted Value', linestyle='-')\n",
    "        plt.title(f'Missing {location}, epochs=200, batch_size=16, patience=30, validation_split=0.2')\n",
    "        plt.xlabel('Index')\n",
    "        plt.ylabel('Value')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        folder_path = 'P:\\SU24\\Imputation\\HY_git\\\\128,256,512,64,64'\n",
    "        filename = f'128,256,512,64,64, 2nd, Missing {location}, epochs=200, batch_size=16, patience=30, validation_split=0.2.png'\n",
    "        full_path = os.path.join(folder_path, filename)\n",
    "        plt.savefig(full_path)\n",
    "        plt.show()\n",
    "        \n",
    "    elif all(value in df_check_after for value in df_miss):\n",
    "        print('\\nAll values in df_miss is in the last !!!')\n",
    "        \n",
    "        last_value_combine = transform_to_multivariate(last_data, size_of_gap)\n",
    "                \n",
    "        df_list = to_df(last_value_combine)\n",
    "\n",
    "        X = np.array(df_list.iloc[:, :-1])\n",
    "        X = scaler.fit_transform(X)\n",
    "        \n",
    "        y = np.array(df_list.iloc[:, -1])\n",
    "        \n",
    "        model = model_combine(X)\n",
    "        \n",
    "        weights_path = os.path.join(weights_folder, f'model_weights_location_{location}.h5')\n",
    "        if os.path.exists(weights_path):\n",
    "            model.load_weights(weights_path)\n",
    "            print(f\"Loaded weights for location {location}\")\n",
    "        else:\n",
    "            model.fit(X, y, epochs=200, batch_size=16, callbacks=[early_stopping], validation_split=0.2)\n",
    "            model.save_weights(weights_path)\n",
    "            print(f\"Saved weights for location {location}\")\n",
    "            \n",
    "            \n",
    "        # model.fit(X, y, epochs=200, batch_size=16, callbacks=[early_stopping], validation_split=0.2)\n",
    "\n",
    "        data_test = df.values.tolist()[nan_index - size_of_gap : nan_index + size_of_gap]\n",
    "        data_test = np.concatenate(data_test).ravel()   \n",
    "\n",
    "        results_last_combine = []\n",
    "        for i in range(len(data_test)//2):\n",
    "            data_last = data_test[i:i+1+size_of_gap]\n",
    "\n",
    "            data_last[size_of_gap] = model.predict(scaler.transform(np.array(data_last[:size_of_gap]).reshape(1,-1)))\n",
    "\n",
    "            results_last_combine.append(data_last[size_of_gap])\n",
    "            \n",
    "        #################################################################\n",
    "        print('\\n', 'result of combine only (last):')                       #\n",
    "        calculate_metrics_for_combine(results_last_combine,'results_combine (LAST)')#\n",
    "        print('\\n')                                                     #\n",
    "        # #################################################################\n",
    "        \n",
    "        df = pd.read_csv('Impute_misvalues_hungyen.csv')\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(df['Waterlevel'].values.tolist()[nan_index:nan_index+size_of_gap], label='Water Level')\n",
    "        plt.plot(results_last_combine, label='Predicted Value', linestyle='-')\n",
    "        plt.title(f'Missing {location}, epochs=200, batch_size=16, patience=30, validation_split=0.2')\n",
    "        plt.xlabel('Index')\n",
    "        plt.ylabel('Value')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        folder_path = 'P:\\SU24\\Imputation\\HY_git\\\\128,256,512,64,64'\n",
    "        filename = f'128,256,512,64,64, 2nd, Missing {location}, epochs=200, batch_size=16, patience=30, validation_split=0.2.png'\n",
    "        full_path = os.path.join(folder_path, filename)\n",
    "        plt.savefig(full_path)\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        Da = data[nan_index + size_of_gap:][::-1]\n",
    "\n",
    "        MDa = transform_to_multivariate(Da, size_of_gap)\n",
    "\n",
    "        df_MDa = to_df(MDa)\n",
    "\n",
    "        X_MDa_train = np.array(df_MDa.iloc[:, :-1])\n",
    "        X_MDa_train = scaler.fit_transform(X_MDa_train)\n",
    "        \n",
    "        y_MDa_train = np.array(df_MDa.iloc[:, -1])\n",
    "\n",
    "        model_MDa = model_combine(X_MDa_train)\n",
    "        \n",
    "        weights_path_after = os.path.join(weights_folder, f'model_weights_after_{location}.h5')\n",
    "        if os.path.exists(weights_path_after):\n",
    "            model_MDa.load_weights(weights_path_after)\n",
    "            print(f\"Loaded weights for after the gap at location {location}\")\n",
    "        else:\n",
    "            model_MDa.fit(X_MDa_train, y_MDa_train, epochs=200, batch_size=16, callbacks=[early_stopping], validation_split=0.2)\n",
    "            model_MDa.save_weights(weights_path_after)\n",
    "            print(f\"Saved weights for after the gap at location {location}\")\n",
    "            \n",
    "            \n",
    "        # model_MDa.fit(X_MDa_train, y_MDa_train, epochs=200, batch_size=16, callbacks=[early_stopping], validation_split=0.2)\n",
    "        \n",
    "        data_test_after = df.values.tolist()[nan_index:nan_index + 2 * size_of_gap ][::-1]\n",
    "        data_test_after = np.concatenate(data_test_after).ravel()   \n",
    "\n",
    "        value_lst_after = []\n",
    "        for j in range(len(data_test_after)//2):\n",
    "            data_after = data_test_after[j:j+1+size_of_gap]\n",
    "            \n",
    "            data_after[size_of_gap] = model_MDa.predict(scaler.transform(np.array(data_after[:size_of_gap]).reshape(1,-1)))\n",
    "            \n",
    "            value_lst_after.append(data_after[size_of_gap])\n",
    "\n",
    "        Db = data[:nan_index]\n",
    "\n",
    "        MDb = transform_to_multivariate(Db, size_of_gap)  \n",
    "\n",
    "        df_MDb = to_df(MDb)\n",
    "\n",
    "        X_MDb_train = np.array(df_MDb.iloc[:, :-1])\n",
    "        X_MDb_train = scaler.fit_transform(X_MDb_train)\n",
    "        \n",
    "        y_MDb_train = np.array(df_MDb.iloc[:, -1])\n",
    "\n",
    "        model_MDb = model_combine(X_MDb_train)\n",
    "        \n",
    "        weights_path_before = os.path.join(weights_folder, f'model_weights_before_{location}.h5')\n",
    "        if os.path.exists(weights_path_before):\n",
    "            model_MDb.load_weights(weights_path_before)\n",
    "            print(f\"Loaded weights for before the gap at location {location}\")\n",
    "        else:\n",
    "            model_MDb.fit(X_MDb_train, y_MDb_train, epochs=200, batch_size=16, callbacks=[early_stopping], validation_split=0.2)\n",
    "            model_MDb.save_weights(weights_path_before)\n",
    "            print(f\"Saved weights for after the gap at location {location}\")\n",
    "            \n",
    "        # model_MDb.fit(X_MDb_train, y_MDb_train, epochs=200, batch_size=16, callbacks=[early_stopping], validation_split=0.2)\n",
    "\n",
    "        data_test_before = df.values.tolist()[nan_index - size_of_gap : nan_index + size_of_gap]\n",
    "        data_test_before = np.concatenate(data_test_before).ravel()   \n",
    "\n",
    "        value_lst_before = []\n",
    "        for i in range(len(data_test_before)//2):\n",
    "            data_before = data_test_before[i:i+1+size_of_gap]\n",
    "            \n",
    "            data_before[size_of_gap] = model_MDb.predict(scaler.transform(np.array(data_before[:size_of_gap]).reshape(1,-1)))\n",
    "\n",
    "            value_lst_before.append(data_before[size_of_gap])\n",
    "\n",
    "        #############################################################################\n",
    "        print('\\n', 'result of combine only: ')                                         #\n",
    "        results_combine =  [(x + y)/2 for x,y in zip(value_lst_before, value_lst_after)]#\n",
    "        calculate_metrics_for_combine(results_combine,'results_combine (MIDDLE)')               #\n",
    "        print('\\n')                                                                 #\n",
    "        #############################################################################\n",
    "        \n",
    "        df = pd.read_csv('Impute_misvalues_hungyen.csv')\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(df['Waterlevel'].values.tolist()[nan_index:nan_index+size_of_gap], label='Water Level')\n",
    "        plt.plot(results_combine, label='Predicted Value', linestyle='-')\n",
    "        plt.title(f'Missing {location}, epochs=200, batch_size=16, patience=30, validation_split=0.2')\n",
    "        plt.xlabel('Index')\n",
    "        plt.ylabel('Value')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()   \n",
    "        folder_path = 'P:\\SU24\\Imputation\\HY_git\\\\128,256,512,64,64'\n",
    "        filename = f'128,256,512,64,64, 2nd, Missing {location}, epochs=200, batch_size=16, patience=30, validation_split=0.2.png'\n",
    "        full_path = os.path.join(folder_path, filename)\n",
    "        plt.savefig(full_path)\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "current_time = datetime.now()\n",
    "\n",
    "print('\\n')\n",
    "print('\\nMean of Similarity combine: ',np.mean(sim_lst_combine))\n",
    "print('\\nMean of Mean Absoulute Error combine :',np.mean(mae_lst_combine))\n",
    "print('\\nMean of Root Mean Squared Error combine: ',np.mean(rmse_lst_combine))\n",
    "print('\\nMean of Fraction of Standard Deviation combine: ',np.mean(fsd_lst_combine)) \n",
    "print('\\nMean of R-score combine: ', np.mean(r_lst_combine))\n",
    "print('\\nMean of the Nash Sutcliffe efficiency (NSE): ', np.mean(nse_lst_combine))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "\n",
      "All values in df_miss is in the first !!!\n",
      "Epoch 1/200\n",
      "3198/3198 [==============================] - 26s 8ms/step - loss: 21.7620 - mae: 3.4140 - val_loss: 237.3962 - val_mae: 8.1279\n",
      "Epoch 2/200\n",
      "3198/3198 [==============================] - 27s 8ms/step - loss: 22.1513 - mae: 3.4283 - val_loss: 190.6691 - val_mae: 7.5576\n",
      "Epoch 3/200\n",
      "3198/3198 [==============================] - 27s 8ms/step - loss: 21.7473 - mae: 3.4071 - val_loss: 186.3320 - val_mae: 6.8875\n",
      "Epoch 4/200\n",
      "3198/3198 [==============================] - 25s 8ms/step - loss: 21.5798 - mae: 3.3966 - val_loss: 142.3781 - val_mae: 6.4930\n",
      "Epoch 5/200\n",
      "1755/3198 [===============>..............] - ETA: 10s - loss: 20.8047 - mae: 3.3529"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 53\u001b[0m\n\u001b[0;32m     50\u001b[0m new_weights_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(weights_folder, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_weights_location_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_1.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     51\u001b[0m model\u001b[38;5;241m.\u001b[39mload_weights(weights_path)\n\u001b[1;32m---> 53\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m model\u001b[38;5;241m.\u001b[39msave_weights(new_weights_path)\n\u001b[0;32m     57\u001b[0m data_test \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist()[nan_index : nan_index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m size_of_gap][::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\thain\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\thain\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\thain\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\thain\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\thain\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\thain\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\thain\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\thain\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\thain\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "# tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "weights_folder = 'model_weights'\n",
    "os.makedirs(weights_folder, exist_ok=True)\n",
    "\n",
    "for location in range(0,12):\n",
    "    \n",
    "    filename = f'waterlevel_missing/waterlevel_missing_{location}.csv'\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    #### Check size of missing value\n",
    "    size_of_gap = df['Waterlevel'].isna().sum()\n",
    "\n",
    "    data = df['Waterlevel'].values.tolist()\n",
    "\n",
    "    nan_index = None\n",
    "    for j, value in enumerate(data):\n",
    "        if value != value:  # Check if the value is NaN\n",
    "            nan_index = j\n",
    "            break\n",
    "        \n",
    "    df_check_before = data[:(3*size_of_gap)+1]\n",
    "    df_check_after = data[::-1][:3*size_of_gap+1]\n",
    "    df_miss = data[nan_index:nan_index + size_of_gap]\n",
    "\n",
    "    last_data = data[:nan_index]\n",
    "    first_data = data[nan_index+size_of_gap:][::-1]\n",
    "\n",
    "    # check if missing values is in the first 3 x T data original\n",
    "    if all(value in df_check_before for value in df_miss):\n",
    "        print('\\nAll values in df_miss is in the first !!!')\n",
    "\n",
    "        first_value_combine = transform_to_multivariate(first_data, size_of_gap)\n",
    "                \n",
    "        df_list = to_df(first_value_combine)\n",
    "        \n",
    "\n",
    "        X = np.array(df_list.iloc[:, :-1])\n",
    "        X = scaler.fit_transform(X)\n",
    "        \n",
    "        y = np.array(df_list.iloc[:, -1])\n",
    "        \n",
    "        model = model_combine(X)\n",
    "        \n",
    "        weights_path = os.path.join(weights_folder, f'model_weights_location_{location}.h5')\n",
    "\n",
    "        new_weights_path = os.path.join(weights_folder, f'model_weights_location_{location}_1.h5')\n",
    "        model.load_weights(weights_path)\n",
    "        \n",
    "        model.fit(X, y, epochs=200, batch_size=16, callbacks=[early_stopping], validation_split=0.2)\n",
    "        model.save_weights(new_weights_path)\n",
    "            \n",
    "            \n",
    "        data_test = df.values.tolist()[nan_index : nan_index + 2 * size_of_gap][::-1]\n",
    "        data_test = np.concatenate(data_test).ravel()   \n",
    "\n",
    "        results_first_combine = []\n",
    "        for i in range(len(data_test)//2):\n",
    "            data_first = data_test[i:i+1+size_of_gap]\n",
    "\n",
    "            data_first[size_of_gap] = model.predict(scaler.transform(np.array(data_first[:size_of_gap]).reshape(1,-1)))\n",
    "\n",
    "            results_first_combine.append(data_first[size_of_gap])\n",
    "            \n",
    "        ###################################################################\n",
    "        print('\\n', 'result of combine only (first):')                        #  \n",
    "        calculate_metrics_for_combine(results_first_combine,'results_combine (FIRST)')#\n",
    "        print('\\n')                                                       #\n",
    "        ###################################################################\n",
    "        \n",
    "        df = pd.read_csv('Impute_misvalues_hungyen.csv')\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(df['Waterlevel'].values.tolist()[nan_index:nan_index+size_of_gap], label='Water Level')\n",
    "        plt.plot(results_first_combine, label='Predicted Value', linestyle='-')\n",
    "        plt.title(f'Missing {location}, epochs=200, batch_size=16, patience=30, validation_split=0.2')\n",
    "        plt.xlabel('Index')\n",
    "        plt.ylabel('Value')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        folder_path = 'P:\\SU24\\Imputation\\HY_git\\\\128,256,512,64,64'\n",
    "        filename = f'128,256,512,64,64, 3nd, Missing {location}, epochs=200, batch_size=16, patience=30, validation_split=0.2.png'\n",
    "        full_path = os.path.join(folder_path, filename)\n",
    "        plt.savefig(full_path)\n",
    "        plt.show()\n",
    "        \n",
    "    elif all(value in df_check_after for value in df_miss):\n",
    "        print('\\nAll values in df_miss is in the last !!!')\n",
    "        \n",
    "        last_value_combine = transform_to_multivariate(last_data, size_of_gap)\n",
    "                \n",
    "        df_list = to_df(last_value_combine)\n",
    "\n",
    "        X = np.array(df_list.iloc[:, :-1])\n",
    "        X = scaler.fit_transform(X)\n",
    "        \n",
    "        y = np.array(df_list.iloc[:, -1])\n",
    "        \n",
    "        model = model_combine(X)\n",
    "        \n",
    "        weights_path = os.path.join(weights_folder, f'model_weights_location_{location}.h5')\n",
    "\n",
    "        new_weights_path = os.path.join(weights_folder, f'model_weights_location_{location}_1.h5')\n",
    "        model.load_weights(weights_path)\n",
    "        \n",
    "        model.fit(X, y, epochs=200, batch_size=16, callbacks=[early_stopping], validation_split=0.2)\n",
    "        model.save_weights(new_weights_path)\n",
    "\n",
    "\n",
    "        data_test = df.values.tolist()[nan_index - size_of_gap : nan_index + size_of_gap]\n",
    "        data_test = np.concatenate(data_test).ravel()   \n",
    "\n",
    "        results_last_combine = []\n",
    "        for i in range(len(data_test)//2):\n",
    "            data_last = data_test[i:i+1+size_of_gap]\n",
    "\n",
    "            data_last[size_of_gap] = model.predict(scaler.transform(np.array(data_last[:size_of_gap]).reshape(1,-1)))\n",
    "\n",
    "            results_last_combine.append(data_last[size_of_gap])\n",
    "            \n",
    "        #################################################################\n",
    "        print('\\n', 'result of combine only (last):')                       #\n",
    "        calculate_metrics_for_combine(results_last_combine,'results_combine (LAST)')#\n",
    "        print('\\n')                                                     #\n",
    "        # #################################################################\n",
    "        \n",
    "        df = pd.read_csv('Impute_misvalues_hungyen.csv')\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(df['Waterlevel'].values.tolist()[nan_index:nan_index+size_of_gap], label='Water Level')\n",
    "        plt.plot(results_last_combine, label='Predicted Value', linestyle='-')\n",
    "        plt.title(f'Missing {location}, epochs=200, batch_size=16, patience=30, validation_split=0.2')\n",
    "        plt.xlabel('Index')\n",
    "        plt.ylabel('Value')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        folder_path = 'P:\\SU24\\Imputation\\HY_git\\\\128,256,512,64,64'\n",
    "        filename = f'128,256,512,64,64, 3nd, Missing {location}, epochs=200, batch_size=16, patience=30, validation_split=0.2.png'\n",
    "        full_path = os.path.join(folder_path, filename)\n",
    "        plt.savefig(full_path)\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        Da = data[nan_index + size_of_gap:][::-1]\n",
    "\n",
    "        MDa = transform_to_multivariate(Da, size_of_gap)\n",
    "\n",
    "        df_MDa = to_df(MDa)\n",
    "\n",
    "        X_MDa_train = np.array(df_MDa.iloc[:, :-1])\n",
    "        X_MDa_train = scaler.fit_transform(X_MDa_train)\n",
    "        \n",
    "        y_MDa_train = np.array(df_MDa.iloc[:, -1])\n",
    "\n",
    "        model_MDa = model_combine(X_MDa_train)\n",
    "            \n",
    "        weights_path_after = os.path.join(weights_folder, f'model_weights_after_{location}.h5')\n",
    "\n",
    "        new_weights_path_after = os.path.join(weights_folder, f'model_weights_after_{location}_1.h5')\n",
    "        model_MDa.load_weights(weights_path_after)\n",
    "        \n",
    "        model_MDa.fit(X_MDa_train, y_MDa_train, epochs=200, batch_size=16, callbacks=[early_stopping], validation_split=0.2)\n",
    "        model_MDa.save_weights(new_weights_path_after)\n",
    "        \n",
    "        data_test_after = df.values.tolist()[nan_index:nan_index + 2 * size_of_gap ][::-1]\n",
    "        data_test_after = np.concatenate(data_test_after).ravel()   \n",
    "\n",
    "        value_lst_after = []\n",
    "        for j in range(len(data_test_after)//2):\n",
    "            data_after = data_test_after[j:j+1+size_of_gap]\n",
    "            \n",
    "            data_after[size_of_gap] = model_MDa.predict(scaler.transform(np.array(data_after[:size_of_gap]).reshape(1,-1)))\n",
    "            \n",
    "            value_lst_after.append(data_after[size_of_gap])\n",
    "\n",
    "        Db = data[:nan_index]\n",
    "\n",
    "        MDb = transform_to_multivariate(Db, size_of_gap)  \n",
    "\n",
    "        df_MDb = to_df(MDb)\n",
    "\n",
    "        X_MDb_train = np.array(df_MDb.iloc[:, :-1])\n",
    "        X_MDb_train = scaler.fit_transform(X_MDb_train)\n",
    "        \n",
    "        y_MDb_train = np.array(df_MDb.iloc[:, -1])\n",
    "\n",
    "        model_MDb = model_combine(X_MDb_train)\n",
    "        \n",
    "        weights_path_before = os.path.join(weights_folder, f'model_weights_before_{location}.h5')\n",
    "\n",
    "        new_weights_path_before = os.path.join(weights_folder, f'model_weights_before_{location}_1.h5')\n",
    "        model_MDb.load_weights(weights_path_before)\n",
    "        \n",
    "        model_MDb.fit(X_MDb_train, y_MDb_train, epochs=200, batch_size=16, callbacks=[early_stopping], validation_split=0.2)\n",
    "        model_MDb.save_weights(new_weights_path_before)\n",
    "\n",
    "        data_test_before = df.values.tolist()[nan_index - size_of_gap : nan_index + size_of_gap]\n",
    "        data_test_before = np.concatenate(data_test_before).ravel()   \n",
    "\n",
    "        value_lst_before = []\n",
    "        for i in range(len(data_test_before)//2):\n",
    "            data_before = data_test_before[i:i+1+size_of_gap]\n",
    "            \n",
    "            data_before[size_of_gap] = model_MDb.predict(scaler.transform(np.array(data_before[:size_of_gap]).reshape(1,-1)))\n",
    "\n",
    "            value_lst_before.append(data_before[size_of_gap])\n",
    "\n",
    "        #############################################################################\n",
    "        print('\\n', 'result of combine only: ')                                         #\n",
    "        results_combine =  [(x + y)/2 for x,y in zip(value_lst_before, value_lst_after)]#\n",
    "        calculate_metrics_for_combine(results_combine,'results_combine (MIDDLE)')               #\n",
    "        print('\\n')                                                                 #\n",
    "        #############################################################################\n",
    "        \n",
    "        df = pd.read_csv('Impute_misvalues_hungyen.csv')\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(df['Waterlevel'].values.tolist()[nan_index:nan_index+size_of_gap], label='Water Level')\n",
    "        plt.plot(results_combine, label='Predicted Value', linestyle='-')\n",
    "        plt.title(f'Missing {location}, epochs=200, batch_size=16, patience=30, validation_split=0.2')\n",
    "        plt.xlabel('Index')\n",
    "        plt.ylabel('Value')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()   \n",
    "        folder_path = 'P:\\SU24\\Imputation\\HY_git\\\\128,256,512,64,64'\n",
    "        filename = f'128,256,512,64,64, 3nd, Missing {location}, epochs=200, batch_size=16, patience=30, validation_split=0.2.png'\n",
    "        full_path = os.path.join(folder_path, filename)\n",
    "        plt.savefig(full_path)\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "current_time = datetime.now()\n",
    "\n",
    "print('\\n')\n",
    "print('\\nMean of Similarity combine: ',np.mean(sim_lst_combine))\n",
    "print('\\nMean of Mean Absoulute Error combine :',np.mean(mae_lst_combine))\n",
    "print('\\nMean of Root Mean Squared Error combine: ',np.mean(rmse_lst_combine))\n",
    "print('\\nMean of Fraction of Standard Deviation combine: ',np.mean(fsd_lst_combine)) \n",
    "print('\\nMean of R-score combine: ', np.mean(r_lst_combine))\n",
    "print('\\nMean of the Nash Sutcliffe efficiency (NSE): ', np.mean(nse_lst_combine))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "# # tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "\n",
    "# for location in range(0,12):\n",
    "    \n",
    "#     filename = f'waterlevel_missing/waterlevel_missing_{location}.csv'\n",
    "#     df = pd.read_csv(filename)\n",
    "    \n",
    "#     #### Check size of missing value\n",
    "#     size_of_gap = df['Waterlevel'].isna().sum()\n",
    "\n",
    "#     data = df['Waterlevel'].values.tolist()\n",
    "\n",
    "#     nan_index = None\n",
    "#     for j, value in enumerate(data):\n",
    "#         if value != value:  # Check if the value is NaN\n",
    "#             nan_index = j\n",
    "#             break\n",
    "        \n",
    "#     df_check_before = data[:(3*size_of_gap)+1]\n",
    "#     df_check_after = data[::-1][:3*size_of_gap+1]\n",
    "#     df_miss = data[nan_index:nan_index + size_of_gap]\n",
    "\n",
    "#     last_data = data[:nan_index]\n",
    "#     first_data = data[nan_index+size_of_gap:][::-1]\n",
    "\n",
    "#     # check if missing values is in the first 3 x T data original\n",
    "#     if all(value in df_check_before for value in df_miss):\n",
    "#         print('\\nAll values in df_miss is in the first !!!')\n",
    "\n",
    "#         first_value_combine = transform_to_multivariate(first_data, size_of_gap)\n",
    "                \n",
    "#         df_list = to_df(first_value_combine)\n",
    "        \n",
    "\n",
    "#         X = np.array(df_list.iloc[:, :-1])\n",
    "#         X = scaler.fit_transform(X)\n",
    "        \n",
    "#         y = np.array(df_list.iloc[:, -1])\n",
    "        \n",
    "#         model = model_combine(X)\n",
    "#         model.fit(X, y, epochs=200, batch_size=16, callbacks=[early_stopping], validation_split=0.2)\n",
    "\n",
    "#         data_test = df.values.tolist()[nan_index : nan_index + 2 * size_of_gap][::-1]\n",
    "#         data_test = np.concatenate(data_test).ravel()   \n",
    "\n",
    "#         results_first_combine = []\n",
    "#         for i in range(len(data_test)//2):\n",
    "#             data_first = data_test[i:i+1+size_of_gap]\n",
    "\n",
    "#             data_first[size_of_gap] = model.predict(scaler.transform(np.array(data_first[:size_of_gap]).reshape(1,-1)))\n",
    "\n",
    "#             results_first_combine.append(data_first[size_of_gap])\n",
    "            \n",
    "#         ###################################################################\n",
    "#         print('\\n', 'result of combine only (first):')                        #  \n",
    "#         calculate_metrics_for_combine(results_first_combine,'results_combine (FIRST)')#\n",
    "#         print('\\n')                                                       #\n",
    "#         ###################################################################\n",
    "        \n",
    "#         df = pd.read_csv('Impute_misvalues_hungyen.csv')\n",
    "\n",
    "#         plt.figure(figsize=(10, 6))\n",
    "#         plt.plot(df['Waterlevel'].values.tolist()[nan_index:nan_index+size_of_gap], label='Water Level')\n",
    "#         plt.plot(results_first_combine, label='Predicted Value', linestyle='-')\n",
    "#         plt.title(f'Missing {location}, epochs=200, batch_size=16, patience=30, validation_split=0.2')\n",
    "#         plt.xlabel('Index')\n",
    "#         plt.ylabel('Value')\n",
    "#         plt.grid(True)\n",
    "#         plt.legend()\n",
    "#         plt.tight_layout()\n",
    "#         folder_path = 'P:\\SU24\\Imputation\\HY_git\\\\128,256,512,128,128'\n",
    "#         filename = f'128,256,512,128,128, Missing {location}, epochs=200, batch_size=16, patience=30, validation_split=0.2.png'\n",
    "#         full_path = os.path.join(folder_path, filename)\n",
    "#         plt.savefig(full_path)\n",
    "#         plt.show()\n",
    "        \n",
    "#     elif all(value in df_check_after for value in df_miss):\n",
    "#         print('\\nAll values in df_miss is in the last !!!')\n",
    "        \n",
    "#         last_value_combine = transform_to_multivariate(last_data, size_of_gap)\n",
    "                \n",
    "#         df_list = to_df(last_value_combine)\n",
    "\n",
    "#         X = np.array(df_list.iloc[:, :-1])\n",
    "#         X = scaler.fit_transform(X)\n",
    "        \n",
    "#         y = np.array(df_list.iloc[:, -1])\n",
    "        \n",
    "#         model = model_combine(X)\n",
    "#         model.fit(X, y, epochs=200, batch_size=16, callbacks=[early_stopping], validation_split=0.2)\n",
    "\n",
    "#         data_test = df.values.tolist()[nan_index - size_of_gap : nan_index + size_of_gap]\n",
    "#         data_test = np.concatenate(data_test).ravel()   \n",
    "\n",
    "#         results_last_combine = []\n",
    "#         for i in range(len(data_test)//2):\n",
    "#             data_last = data_test[i:i+1+size_of_gap]\n",
    "\n",
    "#             data_last[size_of_gap] = model.predict(scaler.transform(np.array(data_last[:size_of_gap]).reshape(1,-1)))\n",
    "\n",
    "#             results_last_combine.append(data_last[size_of_gap])\n",
    "            \n",
    "#         #################################################################\n",
    "#         print('\\n', 'result of combine only (last):')                       #\n",
    "#         calculate_metrics_for_combine(results_last_combine,'results_combine (LAST)')#\n",
    "#         print('\\n')                                                     #\n",
    "#         # #################################################################\n",
    "        \n",
    "#         df = pd.read_csv('Impute_misvalues_hungyen.csv')\n",
    "\n",
    "#         plt.figure(figsize=(10, 6))\n",
    "#         plt.plot(df['Waterlevel'].values.tolist()[nan_index:nan_index+size_of_gap], label='Water Level')\n",
    "#         plt.plot(results_last_combine, label='Predicted Value', linestyle='-')\n",
    "#         plt.title(f'Missing {location}, epochs=200, batch_size=16, patience=30, validation_split=0.2')\n",
    "#         plt.xlabel('Index')\n",
    "#         plt.ylabel('Value')\n",
    "#         plt.grid(True)\n",
    "#         plt.legend()\n",
    "#         plt.tight_layout()\n",
    "#         folder_path = 'P:\\SU24\\Imputation\\HY_git\\\\128,256,512,128,128'\n",
    "#         filename = f'128,256,512,128,128, Missing {location}, epochs=200, batch_size=16, patience=30, validation_split=0.2.png'\n",
    "#         full_path = os.path.join(folder_path, filename)\n",
    "#         plt.savefig(full_path)\n",
    "#         plt.show()\n",
    "        \n",
    "#     else:\n",
    "#         Da = data[nan_index + size_of_gap:][::-1]\n",
    "\n",
    "#         MDa = transform_to_multivariate(Da, size_of_gap)\n",
    "\n",
    "#         df_MDa = to_df(MDa)\n",
    "\n",
    "#         X_MDa_train = np.array(df_MDa.iloc[:, :-1])\n",
    "#         X_MDa_train = scaler.fit_transform(X_MDa_train)\n",
    "        \n",
    "#         y_MDa_train = np.array(df_MDa.iloc[:, -1])\n",
    "\n",
    "#         model_MDa = model_combine(X_MDa_train)\n",
    "#         model_MDa.fit(X_MDa_train, y_MDa_train, epochs=200, batch_size=16, callbacks=[early_stopping], validation_split=0.2)\n",
    "        \n",
    "#         data_test_after = df.values.tolist()[nan_index:nan_index + 2 * size_of_gap ][::-1]\n",
    "#         data_test_after = np.concatenate(data_test_after).ravel()   \n",
    "\n",
    "#         value_lst_after = []\n",
    "#         for j in range(len(data_test_after)//2):\n",
    "#             data_after = data_test_after[j:j+1+size_of_gap]\n",
    "            \n",
    "#             data_after[size_of_gap] = model_MDa.predict(scaler.transform(np.array(data_after[:size_of_gap]).reshape(1,-1)))\n",
    "            \n",
    "#             value_lst_after.append(data_after[size_of_gap])\n",
    "\n",
    "#         Db = data[:nan_index]\n",
    "\n",
    "#         MDb = transform_to_multivariate(Db, size_of_gap)  \n",
    "\n",
    "#         df_MDb = to_df(MDb)\n",
    "\n",
    "#         X_MDb_train = np.array(df_MDb.iloc[:, :-1])\n",
    "#         X_MDb_train = scaler.fit_transform(X_MDb_train)\n",
    "        \n",
    "#         y_MDb_train = np.array(df_MDb.iloc[:, -1])\n",
    "\n",
    "#         model_MDb = model_combine(X_MDb_train)\n",
    "#         model_MDb.fit(X_MDb_train, y_MDb_train, epochs=200, batch_size=16, callbacks=[early_stopping], validation_split=0.2)\n",
    "\n",
    "#         data_test_before = df.values.tolist()[nan_index - size_of_gap : nan_index + size_of_gap]\n",
    "#         data_test_before = np.concatenate(data_test_before).ravel()   \n",
    "\n",
    "#         value_lst_before = []\n",
    "#         for i in range(len(data_test_before)//2):\n",
    "#             data_before = data_test_before[i:i+1+size_of_gap]\n",
    "            \n",
    "#             data_before[size_of_gap] = model_MDb.predict(scaler.transform(np.array(data_before[:size_of_gap]).reshape(1,-1)))\n",
    "\n",
    "#             value_lst_before.append(data_before[size_of_gap])\n",
    "\n",
    "#         #############################################################################\n",
    "#         print('\\n', 'result of combine only: ')                                         #\n",
    "#         results_combine =  [(x + y)/2 for x,y in zip(value_lst_before, value_lst_after)]#\n",
    "#         calculate_metrics_for_combine(results_combine,'results_combine (MIDDLE)')               #\n",
    "#         print('\\n')                                                                 #\n",
    "#         #############################################################################\n",
    "        \n",
    "#         df = pd.read_csv('Impute_misvalues_hungyen.csv')\n",
    "\n",
    "#         plt.figure(figsize=(10, 6))\n",
    "#         plt.plot(df['Waterlevel'].values.tolist()[nan_index:nan_index+size_of_gap], label='Water Level')\n",
    "#         plt.plot(results_combine, label='Predicted Value', linestyle='-')\n",
    "#         plt.title(f'Missing {location}, epochs=200, batch_size=16, patience=30, validation_split=0.2')\n",
    "#         plt.xlabel('Index')\n",
    "#         plt.ylabel('Value')\n",
    "#         plt.grid(True)\n",
    "#         plt.legend()\n",
    "#         plt.tight_layout()   \n",
    "#         folder_path = 'P:\\SU24\\Imputation\\HY_git\\\\128,256,512,128,128'\n",
    "#         filename = f'128,256,512,128,128, Missing {location}, epochs=200, batch_size=16, patience=30, validation_split=0.2.png'\n",
    "#         full_path = os.path.join(folder_path, filename)\n",
    "#         plt.savefig(full_path)\n",
    "#         plt.show()\n",
    "    \n",
    "    \n",
    "# current_time = datetime.now()\n",
    "\n",
    "# print('\\n')\n",
    "# print('\\nMean of Similarity combine: ',np.mean(sim_lst_combine))\n",
    "# print('\\nMean of Mean Absoulute Error combine :',np.mean(mae_lst_combine))\n",
    "# print('\\nMean of Root Mean Squared Error combine: ',np.mean(rmse_lst_combine))\n",
    "# print('\\nMean of Fraction of Standard Deviation combine: ',np.mean(fsd_lst_combine)) \n",
    "# print('\\nMean of R-score combine: ', np.mean(r_lst_combine))\n",
    "# print('\\nMean of the Nash Sutcliffe efficiency (NSE): ', np.mean(nse_lst_combine))\n",
    "# print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### epochs=200, batch_size=8, callbacks (patience=30), validation_split=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "# # tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "\n",
    "# for location in range(0,12):\n",
    "    \n",
    "#     filename = f'waterlevel_missing_{location}.csv'\n",
    "#     df = pd.read_csv(filename)\n",
    "    \n",
    "#     #### Check size of missing value\n",
    "#     size_of_gap = df['Waterlevel'].isna().sum()\n",
    "\n",
    "#     data = df['Waterlevel'].values.tolist()\n",
    "\n",
    "#     nan_index = None\n",
    "#     for j, value in enumerate(data):\n",
    "#         if value != value:  # Check if the value is NaN\n",
    "#             nan_index = j\n",
    "#             break\n",
    "        \n",
    "#     df_check_before = data[:(3*size_of_gap)+1]\n",
    "#     df_check_after = data[::-1][:3*size_of_gap+1]\n",
    "#     df_miss = data[nan_index:nan_index + size_of_gap]\n",
    "\n",
    "#     last_data = data[:nan_index]\n",
    "#     first_data = data[nan_index+size_of_gap:][::-1]\n",
    "\n",
    "#     # check if missing values is in the first 3 x T data original\n",
    "#     if all(value in df_check_before for value in df_miss):\n",
    "#         print('\\nAll values in df_miss is in the first !!!')\n",
    "\n",
    "#         first_value_combine = transform_to_multivariate(first_data, size_of_gap)\n",
    "                \n",
    "#         df_list = to_df(first_value_combine)\n",
    "        \n",
    "\n",
    "#         X = np.array(df_list.iloc[:, :-1])\n",
    "#         X = scaler.fit_transform(X)\n",
    "        \n",
    "#         y = np.array(df_list.iloc[:, -1])\n",
    "        \n",
    "#         model = model_combine(X)\n",
    "#         model.fit(X, y, epochs=200, batch_size=8, callbacks=[early_stopping], validation_split=0.2)\n",
    "\n",
    "#         data_test = df.values.tolist()[nan_index : nan_index + 2 * size_of_gap][::-1]\n",
    "#         data_test = np.concatenate(data_test).ravel()   \n",
    "\n",
    "#         results_first_combine = []\n",
    "#         for i in range(len(data_test)//2):\n",
    "#             data_first = data_test[i:i+1+size_of_gap]\n",
    "\n",
    "#             data_first[size_of_gap] = model.predict(scaler.transform(np.array(data_first[:size_of_gap]).reshape(1,-1)))\n",
    "\n",
    "#             results_first_combine.append(data_first[size_of_gap])\n",
    "            \n",
    "#         ###################################################################\n",
    "#         print('\\n', 'result of combine only (first):')                        #  \n",
    "#         calculate_metrics_for_combine(results_first_combine,'results_combine (FIRST)')#\n",
    "#         print('\\n')                                                       #\n",
    "#         ###################################################################\n",
    "        \n",
    "#         df = pd.read_csv('Impute_misvalues_hungyen.csv')\n",
    "\n",
    "#         plt.figure(figsize=(10, 6))\n",
    "#         plt.plot(df['Waterlevel'].values.tolist()[nan_index:nan_index+size_of_gap], label='Water Level')\n",
    "#         plt.plot(results_first_combine, label='Predicted Value', linestyle='-')\n",
    "#         plt.title(f'Missing {location}, epochs=200, batch_size=8, patience=30, validation_split=0.2')\n",
    "#         plt.xlabel('Index')\n",
    "#         plt.ylabel('Value')\n",
    "#         plt.grid(True)\n",
    "#         plt.legend()\n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig(f'Missing {location}, epochs=200, batch_size=8, patience=30, validation_split=0.2.png')\n",
    "#         plt.show()\n",
    "        \n",
    "#     elif all(value in df_check_after for value in df_miss):\n",
    "#         print('\\nAll values in df_miss is in the last !!!')\n",
    "        \n",
    "#         last_value_combine = transform_to_multivariate(last_data, size_of_gap)\n",
    "                \n",
    "#         df_list = to_df(last_value_combine)\n",
    "\n",
    "#         X = np.array(df_list.iloc[:, :-1])\n",
    "#         X = scaler.fit_transform(X)\n",
    "        \n",
    "#         y = np.array(df_list.iloc[:, -1])\n",
    "        \n",
    "#         model = model_combine(X)\n",
    "#         model.fit(X, y, epochs=200, batch_size=8, callbacks=[early_stopping], validation_split=0.2)\n",
    "\n",
    "#         data_test = df.values.tolist()[nan_index - size_of_gap : nan_index + size_of_gap]\n",
    "#         data_test = np.concatenate(data_test).ravel()   \n",
    "\n",
    "#         results_last_combine = []\n",
    "#         for i in range(len(data_test)//2):\n",
    "#             data_last = data_test[i:i+1+size_of_gap]\n",
    "\n",
    "#             data_last[size_of_gap] = model.predict(scaler.transform(np.array(data_last[:size_of_gap]).reshape(1,-1)))\n",
    "\n",
    "#             results_last_combine.append(data_last[size_of_gap])\n",
    "            \n",
    "#         #################################################################\n",
    "#         print('\\n', 'result of combine only (last):')                       #\n",
    "#         calculate_metrics_for_combine(results_last_combine,'results_combine (LAST)')#\n",
    "#         print('\\n')                                                     #\n",
    "#         # #################################################################\n",
    "        \n",
    "#         df = pd.read_csv('Impute_misvalues_hungyen.csv')\n",
    "\n",
    "#         plt.figure(figsize=(10, 6))\n",
    "#         plt.plot(df['Waterlevel'].values.tolist()[nan_index:nan_index+size_of_gap], label='Water Level')\n",
    "#         plt.plot(results_last_combine, label='Predicted Value', linestyle='-')\n",
    "#         plt.title(f'Missing {location}, epochs=200, batch_size=8, patience=30, validation_split=0.2')\n",
    "#         plt.xlabel('Index')\n",
    "#         plt.ylabel('Value')\n",
    "#         plt.grid(True)\n",
    "#         plt.legend()\n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig(f'Missing {location}, epochs=200, batch_size=8, patience=30, validation_split=0.2.png')\n",
    "#         plt.show()\n",
    "        \n",
    "#     else:\n",
    "#         Da = data[nan_index + size_of_gap:][::-1]\n",
    "\n",
    "#         MDa = transform_to_multivariate(Da, size_of_gap)\n",
    "\n",
    "#         df_MDa = to_df(MDa)\n",
    "\n",
    "#         X_MDa_train = np.array(df_MDa.iloc[:, :-1])\n",
    "#         X_MDa_train = scaler.fit_transform(X_MDa_train)\n",
    "        \n",
    "#         y_MDa_train = np.array(df_MDa.iloc[:, -1])\n",
    "\n",
    "#         model_MDa = model_combine(X_MDa_train)\n",
    "#         model_MDa.fit(X_MDa_train, y_MDa_train, epochs=200, batch_size=8, callbacks=[early_stopping], validation_split=0.2)\n",
    "        \n",
    "#         data_test_after = df.values.tolist()[nan_index:nan_index + 2 * size_of_gap ][::-1]\n",
    "#         data_test_after = np.concatenate(data_test_after).ravel()   \n",
    "\n",
    "#         value_lst_after = []\n",
    "#         for j in range(len(data_test_after)//2):\n",
    "#             data_after = data_test_after[j:j+1+size_of_gap]\n",
    "            \n",
    "#             data_after[size_of_gap] = model_MDa.predict(scaler.transform(np.array(data_after[:size_of_gap]).reshape(1,-1)))\n",
    "            \n",
    "#             value_lst_after.append(data_after[size_of_gap])\n",
    "\n",
    "#         Db = data[:nan_index]\n",
    "\n",
    "#         MDb = transform_to_multivariate(Db, size_of_gap)  \n",
    "\n",
    "#         df_MDb = to_df(MDb)\n",
    "\n",
    "#         X_MDb_train = np.array(df_MDb.iloc[:, :-1])\n",
    "#         X_MDb_train = scaler.fit_transform(X_MDb_train)\n",
    "        \n",
    "#         y_MDb_train = np.array(df_MDb.iloc[:, -1])\n",
    "\n",
    "#         model_MDb = model_combine(X_MDb_train)\n",
    "#         model_MDb.fit(X_MDb_train, y_MDb_train, epochs=200, batch_size=8, callbacks=[early_stopping], validation_split=0.2)\n",
    "\n",
    "#         data_test_before = df.values.tolist()[nan_index - size_of_gap : nan_index + size_of_gap]\n",
    "#         data_test_before = np.concatenate(data_test_before).ravel()   \n",
    "\n",
    "#         value_lst_before = []\n",
    "#         for i in range(len(data_test_before)//2):\n",
    "#             data_before = data_test_before[i:i+1+size_of_gap]\n",
    "            \n",
    "#             data_before[size_of_gap] = model_MDb.predict(scaler.transform(np.array(data_before[:size_of_gap]).reshape(1,-1)))\n",
    "\n",
    "#             value_lst_before.append(data_before[size_of_gap])\n",
    "\n",
    "#         #############################################################################\n",
    "#         print('\\n', 'result of combine only: ')                                         #\n",
    "#         results_combine =  [(x + y)/2 for x,y in zip(value_lst_before, value_lst_after)]#\n",
    "#         calculate_metrics_for_combine(results_combine,'results_combine (MIDDLE)')               #\n",
    "#         print('\\n')                                                                 #\n",
    "#         #############################################################################\n",
    "        \n",
    "#         df = pd.read_csv('Impute_misvalues_hungyen.csv')\n",
    "\n",
    "#         plt.figure(figsize=(10, 6))\n",
    "#         plt.plot(df['Waterlevel'].values.tolist()[nan_index:nan_index+size_of_gap], label='Water Level')\n",
    "#         plt.plot(results_combine, label='Predicted Value', linestyle='-')\n",
    "#         plt.title(f'Missing {location}, epochs=200, batch_size=8, patience=30, validation_split=0.2')\n",
    "#         plt.xlabel('Index')\n",
    "#         plt.ylabel('Value')\n",
    "#         plt.grid(True)\n",
    "#         plt.legend()\n",
    "#         plt.tight_layout()   \n",
    "#         plt.savefig(f'Missing {location}, epochs=200, batch_size=8, patience=30, validation_split=0.2.png')\n",
    "#         plt.show()\n",
    "    \n",
    "    \n",
    "# current_time = datetime.now()\n",
    "\n",
    "# print('\\n')\n",
    "# print('\\nMean of Similarity combine: ',np.mean(sim_lst_combine))\n",
    "# print('\\nMean of Mean Absoulute Error combine :',np.mean(mae_lst_combine))\n",
    "# print('\\nMean of Root Mean Squared Error combine: ',np.mean(rmse_lst_combine))\n",
    "# print('\\nMean of Fraction of Standard Deviation combine: ',np.mean(fsd_lst_combine)) \n",
    "# print('\\nMean of R-score combine: ', np.mean(r_lst_combine))\n",
    "# print('\\nMean of the Nash Sutcliffe efficiency (NSE): ', np.mean(nse_lst_combine))\n",
    "# print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### epochs=200, batch_size=32, callbacks (patience=30), validation_split=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "# # tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "\n",
    "# for location in range(0,12):\n",
    "    \n",
    "#     filename = f'waterlevel_missing_{location}.csv'\n",
    "#     df = pd.read_csv(filename)\n",
    "    \n",
    "#     #### Check size of missing value\n",
    "#     size_of_gap = df['Waterlevel'].isna().sum()\n",
    "\n",
    "#     data = df['Waterlevel'].values.tolist()\n",
    "\n",
    "#     nan_index = None\n",
    "#     for j, value in enumerate(data):\n",
    "#         if value != value:  # Check if the value is NaN\n",
    "#             nan_index = j\n",
    "#             break\n",
    "        \n",
    "#     df_check_before = data[:(3*size_of_gap)+1]\n",
    "#     df_check_after = data[::-1][:3*size_of_gap+1]\n",
    "#     df_miss = data[nan_index:nan_index + size_of_gap]\n",
    "\n",
    "#     last_data = data[:nan_index]\n",
    "#     first_data = data[nan_index+size_of_gap:][::-1]\n",
    "\n",
    "#     # check if missing values is in the first 3 x T data original\n",
    "#     if all(value in df_check_before for value in df_miss):\n",
    "#         print('\\nAll values in df_miss is in the first !!!')\n",
    "\n",
    "#         first_value_combine = transform_to_multivariate(first_data, size_of_gap)\n",
    "                \n",
    "#         df_list = to_df(first_value_combine)\n",
    "        \n",
    "\n",
    "#         X = np.array(df_list.iloc[:, :-1])\n",
    "#         X = scaler.fit_transform(X)\n",
    "        \n",
    "#         y = np.array(df_list.iloc[:, -1])\n",
    "        \n",
    "#         model = model_combine(X)\n",
    "#         model.fit(X, y, epochs=200, batch_size=32, callbacks=[early_stopping], validation_split=0.2)\n",
    "\n",
    "#         data_test = df.values.tolist()[nan_index : nan_index + 2 * size_of_gap][::-1]\n",
    "#         data_test = np.concatenate(data_test).ravel()   \n",
    "\n",
    "#         results_first_combine = []\n",
    "#         for i in range(len(data_test)//2):\n",
    "#             data_first = data_test[i:i+1+size_of_gap]\n",
    "\n",
    "#             data_first[size_of_gap] = model.predict(scaler.transform(np.array(data_first[:size_of_gap]).reshape(1,-1)))\n",
    "\n",
    "#             results_first_combine.append(data_first[size_of_gap])\n",
    "            \n",
    "#         ###################################################################\n",
    "#         print('\\n', 'result of combine only (first):')                        #  \n",
    "#         calculate_metrics_for_combine(results_first_combine,'results_combine (FIRST)')#\n",
    "#         print('\\n')                                                       #\n",
    "#         ###################################################################\n",
    "        \n",
    "#         df = pd.read_csv('Impute_misvalues_hungyen.csv')\n",
    "\n",
    "#         plt.figure(figsize=(10, 6))\n",
    "#         plt.plot(df['Waterlevel'].values.tolist()[nan_index:nan_index+size_of_gap], label='Water Level')\n",
    "#         plt.plot(results_first_combine, label='Predicted Value', linestyle='-')\n",
    "#         plt.title(f'Water Level vs Predicted Value, mising {location}, epochs=200, batch_size=32, patience=30, validation_split=0.2')\n",
    "#         plt.xlabel('Index')\n",
    "#         plt.ylabel('Value')\n",
    "#         plt.grid(True)\n",
    "#         plt.legend()\n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig(f'Missing {location}, epochs=200, batch_size=32, patience=30, validation_split=0.2.png')\n",
    "#         plt.show()\n",
    "        \n",
    "#     elif all(value in df_check_after for value in df_miss):\n",
    "#         print('\\nAll values in df_miss is in the last !!!')\n",
    "        \n",
    "#         last_value_combine = transform_to_multivariate(last_data, size_of_gap)\n",
    "                \n",
    "#         df_list = to_df(last_value_combine)\n",
    "\n",
    "#         X = np.array(df_list.iloc[:, :-1])\n",
    "#         X = scaler.fit_transform(X)\n",
    "        \n",
    "#         y = np.array(df_list.iloc[:, -1])\n",
    "        \n",
    "#         model = model_combine(X)\n",
    "#         model.fit(X, y, epochs=200, batch_size=32, callbacks=[early_stopping], validation_split=0.2)\n",
    "\n",
    "#         data_test = df.values.tolist()[nan_index - size_of_gap : nan_index + size_of_gap]\n",
    "#         data_test = np.concatenate(data_test).ravel()   \n",
    "\n",
    "#         results_last_combine = []\n",
    "#         for i in range(len(data_test)//2):\n",
    "#             data_last = data_test[i:i+1+size_of_gap]\n",
    "\n",
    "#             data_last[size_of_gap] = model.predict(scaler.transform(np.array(data_last[:size_of_gap]).reshape(1,-1)))\n",
    "\n",
    "#             results_last_combine.append(data_last[size_of_gap])\n",
    "            \n",
    "#         #################################################################\n",
    "#         print('\\n', 'result of combine only (last):')                       #\n",
    "#         calculate_metrics_for_combine(results_last_combine,'results_combine (LAST)')#\n",
    "#         print('\\n')                                                     #\n",
    "#         # #################################################################\n",
    "        \n",
    "#         df = pd.read_csv('Impute_misvalues_hungyen.csv')\n",
    "\n",
    "#         plt.figure(figsize=(10, 6))\n",
    "#         plt.plot(df['Waterlevel'].values.tolist()[nan_index:nan_index+size_of_gap], label='Water Level')\n",
    "#         plt.plot(results_last_combine, label='Predicted Value', linestyle='-')\n",
    "#         plt.title(f'Water Level vs Predicted Value, mising {location}, epochs=200, batch_size=32, patience=30, validation_split=0.2')\n",
    "#         plt.xlabel('Index')\n",
    "#         plt.ylabel('Value')\n",
    "#         plt.grid(True)\n",
    "#         plt.legend()\n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig(f'Missing {location}, epochs=200, batch_size=32, patience=30, validation_split=0.2.png')\n",
    "#         plt.show()\n",
    "        \n",
    "#     else:\n",
    "#         Da = data[nan_index + size_of_gap:][::-1]\n",
    "\n",
    "#         MDa = transform_to_multivariate(Da, size_of_gap)\n",
    "\n",
    "#         df_MDa = to_df(MDa)\n",
    "\n",
    "#         X_MDa_train = np.array(df_MDa.iloc[:, :-1])\n",
    "#         X_MDa_train = scaler.fit_transform(X_MDa_train)\n",
    "        \n",
    "#         y_MDa_train = np.array(df_MDa.iloc[:, -1])\n",
    "\n",
    "#         model_MDa = model_combine(X_MDa_train)\n",
    "#         model_MDa.fit(X_MDa_train, y_MDa_train, epochs=200, batch_size=32, callbacks=[early_stopping], validation_split=0.2)\n",
    "        \n",
    "#         data_test_after = df.values.tolist()[nan_index:nan_index + 2 * size_of_gap ][::-1]\n",
    "#         data_test_after = np.concatenate(data_test_after).ravel()   \n",
    "\n",
    "#         value_lst_after = []\n",
    "#         for j in range(len(data_test_after)//2):\n",
    "#             data_after = data_test_after[j:j+1+size_of_gap]\n",
    "            \n",
    "#             data_after[size_of_gap] = model_MDa.predict(scaler.transform(np.array(data_after[:size_of_gap]).reshape(1,-1)))\n",
    "            \n",
    "#             value_lst_after.append(data_after[size_of_gap])\n",
    "\n",
    "#         Db = data[:nan_index]\n",
    "\n",
    "#         MDb = transform_to_multivariate(Db, size_of_gap)  \n",
    "\n",
    "#         df_MDb = to_df(MDb)\n",
    "\n",
    "#         X_MDb_train = np.array(df_MDb.iloc[:, :-1])\n",
    "#         X_MDb_train = scaler.fit_transform(X_MDb_train)\n",
    "        \n",
    "#         y_MDb_train = np.array(df_MDb.iloc[:, -1])\n",
    "\n",
    "#         model_MDb = model_combine(X_MDb_train)\n",
    "#         model_MDb.fit(X_MDb_train, y_MDb_train, epochs=200, batch_size=32, callbacks=[early_stopping], validation_split=0.2)\n",
    "\n",
    "#         data_test_before = df.values.tolist()[nan_index - size_of_gap : nan_index + size_of_gap]\n",
    "#         data_test_before = np.concatenate(data_test_before).ravel()   \n",
    "\n",
    "#         value_lst_before = []\n",
    "#         for i in range(len(data_test_before)//2):\n",
    "#             data_before = data_test_before[i:i+1+size_of_gap]\n",
    "            \n",
    "#             data_before[size_of_gap] = model_MDb.predict(scaler.transform(np.array(data_before[:size_of_gap]).reshape(1,-1)))\n",
    "\n",
    "#             value_lst_before.append(data_before[size_of_gap])\n",
    "\n",
    "#         #############################################################################\n",
    "#         print('\\n', 'result of combine only: ')                                         #\n",
    "#         results_combine =  [(x + y)/2 for x,y in zip(value_lst_before, value_lst_after)]#\n",
    "#         calculate_metrics_for_combine(results_combine,'results_combine (MIDDLE)')               #\n",
    "#         print('\\n')                                                                 #\n",
    "#         #############################################################################\n",
    "        \n",
    "#         df = pd.read_csv('Impute_misvalues_hungyen.csv')\n",
    "\n",
    "#         plt.figure(figsize=(10, 6))\n",
    "#         plt.plot(df['Waterlevel'].values.tolist()[nan_index:nan_index+size_of_gap], label='Water Level')\n",
    "#         plt.plot(results_combine, label='Predicted Value', linestyle='-')\n",
    "#         plt.title(f'Water Level vs Predicted Value, mising {location}, epochs=200, batch_size=32, patience=30, validation_split=0.2')\n",
    "#         plt.xlabel('Index')\n",
    "#         plt.ylabel('Value')\n",
    "#         plt.grid(True)\n",
    "#         plt.legend()\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()    \n",
    "#         plt.savefig(f'Missing {location}, epochs=200, batch_size=32, patience=30, validation_split=0.2.png')\n",
    "    \n",
    "    \n",
    "# current_time = datetime.now()\n",
    "\n",
    "# print('\\n')\n",
    "# print('\\nMean of Similarity combine: ',np.mean(sim_lst_combine))\n",
    "# print('\\nMean of Mean Absoulute Error combine :',np.mean(mae_lst_combine))\n",
    "# print('\\nMean of Root Mean Squared Error combine: ',np.mean(rmse_lst_combine))\n",
    "# print('\\nMean of Fraction of Standard Deviation combine: ',np.mean(fsd_lst_combine)) \n",
    "# print('\\nMean of R-score combine: ', np.mean(r_lst_combine))\n",
    "# print('\\nMean of the Nash Sutcliffe efficiency (NSE): ', np.mean(nse_lst_combine))\n",
    "# print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### epochs=200, batch_size=8, callbacks (patience=50), validation_split=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "# # tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "\n",
    "\n",
    "# for location in range(0,12):\n",
    "    \n",
    "#     filename = f'waterlevel_missing_{location}.csv'\n",
    "#     df = pd.read_csv(filename)\n",
    "    \n",
    "#     #### Check size of missing value\n",
    "#     size_of_gap = df['Waterlevel'].isna().sum()\n",
    "\n",
    "#     data = df['Waterlevel'].values.tolist()\n",
    "\n",
    "#     nan_index = None\n",
    "#     for j, value in enumerate(data):\n",
    "#         if value != value:  # Check if the value is NaN\n",
    "#             nan_index = j\n",
    "#             break\n",
    "        \n",
    "#     df_check_before = data[:(3*size_of_gap)+1]\n",
    "#     df_check_after = data[::-1][:3*size_of_gap+1]\n",
    "#     df_miss = data[nan_index:nan_index + size_of_gap]\n",
    "\n",
    "#     last_data = data[:nan_index]\n",
    "#     first_data = data[nan_index+size_of_gap:][::-1]\n",
    "\n",
    "#     # check if missing values is in the first 3 x T data original\n",
    "#     if all(value in df_check_before for value in df_miss):\n",
    "#         print('\\nAll values in df_miss is in the first !!!')\n",
    "\n",
    "#         first_value_combine = transform_to_multivariate(first_data, size_of_gap)\n",
    "                \n",
    "#         df_list = to_df(first_value_combine)\n",
    "        \n",
    "\n",
    "#         X = np.array(df_list.iloc[:, :-1])\n",
    "#         X = scaler.fit_transform(X)\n",
    "        \n",
    "#         y = np.array(df_list.iloc[:, -1])\n",
    "        \n",
    "#         model = model_combine(X)\n",
    "#         model.fit(X, y, epochs=200, batch_size=8, callbacks=[early_stopping], validation_split=0.2)\n",
    "\n",
    "#         data_test = df.values.tolist()[nan_index : nan_index + 2 * size_of_gap][::-1]\n",
    "#         data_test = np.concatenate(data_test).ravel()   \n",
    "\n",
    "#         results_first_combine = []\n",
    "#         for i in range(len(data_test)//2):\n",
    "#             data_first = data_test[i:i+1+size_of_gap]\n",
    "\n",
    "#             data_first[size_of_gap] = model.predict(scaler.transform(np.array(data_first[:size_of_gap]).reshape(1,-1)))\n",
    "\n",
    "#             results_first_combine.append(data_first[size_of_gap])\n",
    "            \n",
    "#         ###################################################################\n",
    "#         print('\\n', 'result of combine only (first):')                        #  \n",
    "#         calculate_metrics_for_combine(results_first_combine,'results_combine (FIRST)')#\n",
    "#         print('\\n')                                                       #\n",
    "#         ###################################################################\n",
    "        \n",
    "#         df = pd.read_csv('Impute_misvalues_hungyen.csv')\n",
    "\n",
    "#         plt.figure(figsize=(10, 6))\n",
    "#         plt.plot(df['Waterlevel'].values.tolist()[nan_index:nan_index+size_of_gap], label='Water Level')\n",
    "#         plt.plot(results_first_combine, label='Predicted Value', linestyle='-')\n",
    "#         plt.title(f'Water Level vs Predicted Value, mising {location}, epochs=200, batch_size=8, patience=50, validation_split=0.2')\n",
    "#         plt.xlabel('Index')\n",
    "#         plt.ylabel('Value')\n",
    "#         plt.grid(True)\n",
    "#         plt.legend()\n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig(f'Missing {location}, epochs=200, batch_size=8, patience=50, validation_split=0.2.png')\n",
    "#         plt.show()\n",
    "        \n",
    "#     elif all(value in df_check_after for value in df_miss):\n",
    "#         print('\\nAll values in df_miss is in the last !!!')\n",
    "        \n",
    "#         last_value_combine = transform_to_multivariate(last_data, size_of_gap)\n",
    "                \n",
    "#         df_list = to_df(last_value_combine)\n",
    "\n",
    "#         X = np.array(df_list.iloc[:, :-1])\n",
    "#         X = scaler.fit_transform(X)\n",
    "        \n",
    "#         y = np.array(df_list.iloc[:, -1])\n",
    "        \n",
    "#         model = model_combine(X)\n",
    "#         model.fit(X, y, epochs=200, batch_size=8, callbacks=[early_stopping], validation_split=0.2)\n",
    "\n",
    "#         data_test = df.values.tolist()[nan_index - size_of_gap : nan_index + size_of_gap]\n",
    "#         data_test = np.concatenate(data_test).ravel()   \n",
    "\n",
    "#         results_last_combine = []\n",
    "#         for i in range(len(data_test)//2):\n",
    "#             data_last = data_test[i:i+1+size_of_gap]\n",
    "\n",
    "#             data_last[size_of_gap] = model.predict(scaler.transform(np.array(data_last[:size_of_gap]).reshape(1,-1)))\n",
    "\n",
    "#             results_last_combine.append(data_last[size_of_gap])\n",
    "            \n",
    "#         #################################################################\n",
    "#         print('\\n', 'result of combine only (last):')                       #\n",
    "#         calculate_metrics_for_combine(results_last_combine,'results_combine (LAST)')#\n",
    "#         print('\\n')                                                     #\n",
    "#         # #################################################################\n",
    "        \n",
    "#         df = pd.read_csv('Impute_misvalues_hungyen.csv')\n",
    "\n",
    "#         plt.figure(figsize=(10, 6))\n",
    "#         plt.plot(df['Waterlevel'].values.tolist()[nan_index:nan_index+size_of_gap], label='Water Level')\n",
    "#         plt.plot(results_last_combine, label='Predicted Value', linestyle='-')\n",
    "#         plt.title(f'Water Level vs Predicted Value, mising {location}, epochs=200, batch_size=8, patience=50, validation_split=0.2')\n",
    "#         plt.xlabel('Index')\n",
    "#         plt.ylabel('Value')\n",
    "#         plt.grid(True)\n",
    "#         plt.legend()\n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig(f'Missing {location}, epochs=200, batch_size=8, patience=50, validation_split=0.2.png')\n",
    "#         plt.show()\n",
    "        \n",
    "#     else:\n",
    "#         Da = data[nan_index + size_of_gap:][::-1]\n",
    "\n",
    "#         MDa = transform_to_multivariate(Da, size_of_gap)\n",
    "\n",
    "#         df_MDa = to_df(MDa)\n",
    "\n",
    "#         X_MDa_train = np.array(df_MDa.iloc[:, :-1])\n",
    "#         X_MDa_train = scaler.fit_transform(X_MDa_train)\n",
    "        \n",
    "#         y_MDa_train = np.array(df_MDa.iloc[:, -1])\n",
    "\n",
    "#         model_MDa = model_combine(X_MDa_train)\n",
    "#         model_MDa.fit(X_MDa_train, y_MDa_train, epochs=200, batch_size=8, callbacks=[early_stopping], validation_split=0.2)\n",
    "        \n",
    "#         data_test_after = df.values.tolist()[nan_index:nan_index + 2 * size_of_gap ][::-1]\n",
    "#         data_test_after = np.concatenate(data_test_after).ravel()   \n",
    "\n",
    "#         value_lst_after = []\n",
    "#         for j in range(len(data_test_after)//2):\n",
    "#             data_after = data_test_after[j:j+1+size_of_gap]\n",
    "            \n",
    "#             data_after[size_of_gap] = model_MDa.predict(scaler.transform(np.array(data_after[:size_of_gap]).reshape(1,-1)))\n",
    "            \n",
    "#             value_lst_after.append(data_after[size_of_gap])\n",
    "\n",
    "#         Db = data[:nan_index]\n",
    "\n",
    "#         MDb = transform_to_multivariate(Db, size_of_gap)  \n",
    "\n",
    "#         df_MDb = to_df(MDb)\n",
    "\n",
    "#         X_MDb_train = np.array(df_MDb.iloc[:, :-1])\n",
    "#         X_MDb_train = scaler.fit_transform(X_MDb_train)\n",
    "        \n",
    "#         y_MDb_train = np.array(df_MDb.iloc[:, -1])\n",
    "\n",
    "#         model_MDb = model_combine(X_MDb_train)\n",
    "#         model_MDb.fit(X_MDb_train, y_MDb_train, epochs=200, batch_size=8, callbacks=[early_stopping], validation_split=0.2)\n",
    "\n",
    "#         data_test_before = df.values.tolist()[nan_index - size_of_gap : nan_index + size_of_gap]\n",
    "#         data_test_before = np.concatenate(data_test_before).ravel()   \n",
    "\n",
    "#         value_lst_before = []\n",
    "#         for i in range(len(data_test_before)//2):\n",
    "#             data_before = data_test_before[i:i+1+size_of_gap]\n",
    "            \n",
    "#             data_before[size_of_gap] = model_MDb.predict(scaler.transform(np.array(data_before[:size_of_gap]).reshape(1,-1)))\n",
    "\n",
    "#             value_lst_before.append(data_before[size_of_gap])\n",
    "\n",
    "#         #############################################################################\n",
    "#         print('\\n', 'result of combine only: ')                                         #\n",
    "#         results_combine =  [(x + y)/2 for x,y in zip(value_lst_before, value_lst_after)]#\n",
    "#         calculate_metrics_for_combine(results_combine,'results_combine (MIDDLE)')               #\n",
    "#         print('\\n')                                                                 #\n",
    "#         #############################################################################\n",
    "        \n",
    "#         df = pd.read_csv('Impute_misvalues_hungyen.csv')\n",
    "\n",
    "#         plt.figure(figsize=(10, 6))\n",
    "#         plt.plot(df['Waterlevel'].values.tolist()[nan_index:nan_index+size_of_gap], label='Water Level')\n",
    "#         plt.plot(results_combine, label='Predicted Value', linestyle='-')\n",
    "#         plt.title(f'Water Level vs Predicted Value, mising {location}, epochs=200, batch_size=8, patience=50, validation_split=0.2')\n",
    "#         plt.xlabel('Index')\n",
    "#         plt.ylabel('Value')\n",
    "#         plt.grid(True)\n",
    "#         plt.legend()\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()    \n",
    "#         plt.savefig(f'Missing {location}, epochs=200, batch_size=8, patience=50, validation_split=0.2.png')\n",
    "    \n",
    "    \n",
    "# current_time = datetime.now()\n",
    "\n",
    "# print('\\n')\n",
    "# print('\\nMean of Similarity combine: ',np.mean(sim_lst_combine))\n",
    "# print('\\nMean of Mean Absoulute Error combine :',np.mean(mae_lst_combine))\n",
    "# print('\\nMean of Root Mean Squared Error combine: ',np.mean(rmse_lst_combine))\n",
    "# print('\\nMean of Fraction of Standard Deviation combine: ',np.mean(fsd_lst_combine)) \n",
    "# print('\\nMean of R-score combine: ', np.mean(r_lst_combine))\n",
    "# print('\\nMean of the Nash Sutcliffe efficiency (NSE): ', np.mean(nse_lst_combine))\n",
    "# print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### epochs=200, batch_size=16, callbacks (patience=50), validation_split=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "# # tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "\n",
    "\n",
    "# for location in range(0,12):\n",
    "    \n",
    "#     filename = f'P:\\SU24\\Imputation\\HY_git\\waterlevel_missing\\waterlevel_missing_{location}.csv'\n",
    "#     df = pd.read_csv(filename)\n",
    "    \n",
    "#     #### Check size of missing value\n",
    "#     size_of_gap = df['Waterlevel'].isna().sum()\n",
    "\n",
    "#     data = df['Waterlevel'].values.tolist()\n",
    "\n",
    "#     nan_index = None\n",
    "#     for j, value in enumerate(data):\n",
    "#         if value != value:  # Check if the value is NaN\n",
    "#             nan_index = j\n",
    "#             break\n",
    "        \n",
    "#     df_check_before = data[:(3*size_of_gap)+1]\n",
    "#     df_check_after = data[::-1][:3*size_of_gap+1]\n",
    "#     df_miss = data[nan_index:nan_index + size_of_gap]\n",
    "\n",
    "#     last_data = data[:nan_index]\n",
    "#     first_data = data[nan_index+size_of_gap:][::-1]\n",
    "\n",
    "#     # check if missing values is in the first 3 x T data original\n",
    "#     if all(value in df_check_before for value in df_miss):\n",
    "#         print('\\nAll values in df_miss is in the first !!!')\n",
    "\n",
    "#         first_value_combine = transform_to_multivariate(first_data, size_of_gap)\n",
    "                \n",
    "#         df_list = to_df(first_value_combine)\n",
    "        \n",
    "\n",
    "#         X = np.array(df_list.iloc[:, :-1])\n",
    "#         X = scaler.fit_transform(X)\n",
    "        \n",
    "#         y = np.array(df_list.iloc[:, -1])\n",
    "        \n",
    "#         model = model_combine(X)\n",
    "#         model.fit(X, y, epochs=200, batch_size=16, callbacks=[early_stopping], validation_split=0.2)\n",
    "\n",
    "#         data_test = df.values.tolist()[nan_index : nan_index + 2 * size_of_gap][::-1]\n",
    "#         data_test = np.concatenate(data_test).ravel()   \n",
    "\n",
    "#         results_first_combine = []\n",
    "#         for i in range(len(data_test)//2):\n",
    "#             data_first = data_test[i:i+1+size_of_gap]\n",
    "\n",
    "#             data_first[size_of_gap] = model.predict(scaler.transform(np.array(data_first[:size_of_gap]).reshape(1,-1)))\n",
    "\n",
    "#             results_first_combine.append(data_first[size_of_gap])\n",
    "            \n",
    "#         ###################################################################\n",
    "#         print('\\n', 'result of combine only (first):')                        #  \n",
    "#         calculate_metrics_for_combine(results_first_combine,'results_combine (FIRST)')#\n",
    "#         print('\\n')                                                       #\n",
    "#         ###################################################################\n",
    "        \n",
    "#         df = pd.read_csv('Impute_misvalues_hungyen.csv')\n",
    "\n",
    "#         plt.figure(figsize=(10, 6))\n",
    "#         plt.plot(df['Waterlevel'].values.tolist()[nan_index:nan_index+size_of_gap], label='Water Level')\n",
    "#         plt.plot(results_first_combine, label='Predicted Value', linestyle='-')\n",
    "#         plt.title(f'Mising {location}, epochs=200, batch_size=16, patience=50, validation_split=0.2')\n",
    "#         plt.xlabel('Index')\n",
    "#         plt.ylabel('Value')\n",
    "#         plt.grid(True)\n",
    "#         plt.legend()\n",
    "#         plt.tight_layout()\n",
    "#         folder_path = 'P:\\SU24\\Imputation\\HY_git\\\\128,256,512,64,64'\n",
    "#         filename = f'128,256,512,64,64, Missing {location}, epochs=200, batch_size=16, patience=50, validation_split=0.2.png'\n",
    "#         full_path = os.path.join(folder_path, filename)\n",
    "#         plt.savefig(full_path)\n",
    "#         plt.show()\n",
    "        \n",
    "#     elif all(value in df_check_after for value in df_miss):\n",
    "#         print('\\nAll values in df_miss is in the last !!!')\n",
    "        \n",
    "#         last_value_combine = transform_to_multivariate(last_data, size_of_gap)\n",
    "                \n",
    "#         df_list = to_df(last_value_combine)\n",
    "\n",
    "#         X = np.array(df_list.iloc[:, :-1])\n",
    "#         X = scaler.fit_transform(X)\n",
    "        \n",
    "#         y = np.array(df_list.iloc[:, -1])\n",
    "        \n",
    "#         model = model_combine(X)\n",
    "#         model.fit(X, y, epochs=200, batch_size=16, callbacks=[early_stopping], validation_split=0.2)\n",
    "\n",
    "#         data_test = df.values.tolist()[nan_index - size_of_gap : nan_index + size_of_gap]\n",
    "#         data_test = np.concatenate(data_test).ravel()   \n",
    "\n",
    "#         results_last_combine = []\n",
    "#         for i in range(len(data_test)//2):\n",
    "#             data_last = data_test[i:i+1+size_of_gap]\n",
    "\n",
    "#             data_last[size_of_gap] = model.predict(scaler.transform(np.array(data_last[:size_of_gap]).reshape(1,-1)))\n",
    "\n",
    "#             results_last_combine.append(data_last[size_of_gap])\n",
    "            \n",
    "#         #################################################################\n",
    "#         print('\\n', 'result of combine only (last):')                       #\n",
    "#         calculate_metrics_for_combine(results_last_combine,'results_combine (LAST)')#\n",
    "#         print('\\n')                                                     #\n",
    "#         # #################################################################\n",
    "        \n",
    "#         df = pd.read_csv('Impute_misvalues_hungyen.csv')\n",
    "\n",
    "#         plt.figure(figsize=(10, 6))\n",
    "#         plt.plot(df['Waterlevel'].values.tolist()[nan_index:nan_index+size_of_gap], label='Water Level')\n",
    "#         plt.plot(results_last_combine, label='Predicted Value', linestyle='-')\n",
    "#         plt.title(f'Mising {location}, epochs=200, batch_size=16, patience=50, validation_split=0.2')\n",
    "#         plt.xlabel('Index')\n",
    "#         plt.ylabel('Value')\n",
    "#         plt.grid(True)\n",
    "#         plt.legend()\n",
    "#         plt.tight_layout()\n",
    "#         folder_path = 'P:\\SU24\\Imputation\\HY_git\\\\128,256,512,64,64'\n",
    "#         filename = f'128,256,512,64,64, Missing {location}, epochs=200, batch_size=16, patience=50, validation_split=0.2.png'\n",
    "#         full_path = os.path.join(folder_path, filename)\n",
    "#         plt.savefig(full_path)\n",
    "#         plt.show()\n",
    "        \n",
    "#     else:\n",
    "#         Da = data[nan_index + size_of_gap:][::-1]\n",
    "\n",
    "#         MDa = transform_to_multivariate(Da, size_of_gap)\n",
    "\n",
    "#         df_MDa = to_df(MDa)\n",
    "\n",
    "#         X_MDa_train = np.array(df_MDa.iloc[:, :-1])\n",
    "#         X_MDa_train = scaler.fit_transform(X_MDa_train)\n",
    "        \n",
    "#         y_MDa_train = np.array(df_MDa.iloc[:, -1])\n",
    "\n",
    "#         model_MDa = model_combine(X_MDa_train)\n",
    "#         model_MDa.fit(X_MDa_train, y_MDa_train, epochs=200, batch_size=16, callbacks=[early_stopping], validation_split=0.2)\n",
    "        \n",
    "#         data_test_after = df.values.tolist()[nan_index:nan_index + 2 * size_of_gap ][::-1]\n",
    "#         data_test_after = np.concatenate(data_test_after).ravel()   \n",
    "\n",
    "#         value_lst_after = []\n",
    "#         for j in range(len(data_test_after)//2):\n",
    "#             data_after = data_test_after[j:j+1+size_of_gap]\n",
    "            \n",
    "#             data_after[size_of_gap] = model_MDa.predict(scaler.transform(np.array(data_after[:size_of_gap]).reshape(1,-1)))\n",
    "            \n",
    "#             value_lst_after.append(data_after[size_of_gap])\n",
    "\n",
    "#         Db = data[:nan_index]\n",
    "\n",
    "#         MDb = transform_to_multivariate(Db, size_of_gap)  \n",
    "\n",
    "#         df_MDb = to_df(MDb)\n",
    "\n",
    "#         X_MDb_train = np.array(df_MDb.iloc[:, :-1])\n",
    "#         X_MDb_train = scaler.fit_transform(X_MDb_train)\n",
    "        \n",
    "#         y_MDb_train = np.array(df_MDb.iloc[:, -1])\n",
    "\n",
    "#         model_MDb = model_combine(X_MDb_train)\n",
    "#         model_MDb.fit(X_MDb_train, y_MDb_train, epochs=200, batch_size=16, callbacks=[early_stopping], validation_split=0.2)\n",
    "\n",
    "#         data_test_before = df.values.tolist()[nan_index - size_of_gap : nan_index + size_of_gap]\n",
    "#         data_test_before = np.concatenate(data_test_before).ravel()   \n",
    "\n",
    "#         value_lst_before = []\n",
    "#         for i in range(len(data_test_before)//2):\n",
    "#             data_before = data_test_before[i:i+1+size_of_gap]\n",
    "            \n",
    "#             data_before[size_of_gap] = model_MDb.predict(scaler.transform(np.array(data_before[:size_of_gap]).reshape(1,-1)))\n",
    "\n",
    "#             value_lst_before.append(data_before[size_of_gap])\n",
    "\n",
    "#         #############################################################################\n",
    "#         print('\\n', 'result of combine only: ')                                         #\n",
    "#         results_combine =  [(x + y)/2 for x,y in zip(value_lst_before, value_lst_after)]#\n",
    "#         calculate_metrics_for_combine(results_combine,'results_combine (MIDDLE)')               #\n",
    "#         print('\\n')                                                                 #\n",
    "#         #############################################################################\n",
    "        \n",
    "#         df = pd.read_csv('Impute_misvalues_hungyen.csv')\n",
    "\n",
    "#         plt.figure(figsize=(10, 6))\n",
    "#         plt.plot(df['Waterlevel'].values.tolist()[nan_index:nan_index+size_of_gap], label='Water Level')\n",
    "#         plt.plot(results_combine, label='Predicted Value', linestyle='-')\n",
    "#         plt.title(f'Mising {location}, epochs=200, batch_size=16, patience=50, validation_split=0.2')\n",
    "#         plt.xlabel('Index')\n",
    "#         plt.ylabel('Value')\n",
    "#         plt.grid(True)\n",
    "#         plt.legend()\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()    \n",
    "#         folder_path = 'P:\\SU24\\Imputation\\HY_git\\\\128,256,512,64,64'\n",
    "#         filename = f'128,256,512,64,64, Missing {location}, epochs=200, batch_size=16, patience=50, validation_split=0.2.png'\n",
    "#         full_path = os.path.join(folder_path, filename)\n",
    "#         plt.savefig(full_path)\n",
    "    \n",
    "    \n",
    "# current_time = datetime.now()\n",
    "\n",
    "# print('\\n')\n",
    "# print('\\nMean of Similarity combine: ',np.mean(sim_lst_combine))\n",
    "# print('\\nMean of Mean Absoulute Error combine :',np.mean(mae_lst_combine))\n",
    "# print('\\nMean of Root Mean Squared Error combine: ',np.mean(rmse_lst_combine))\n",
    "# print('\\nMean of Fraction of Standard Deviation combine: ',np.mean(fsd_lst_combine)) \n",
    "# print('\\nMean of R-score combine: ', np.mean(r_lst_combine))\n",
    "# print('\\nMean of the Nash Sutcliffe efficiency (NSE): ', np.mean(nse_lst_combine))\n",
    "# print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### epochs=200, batch_size=32, callbacks (patience=50), validation_split=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "# # tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "\n",
    "\n",
    "# for location in range(0,12):\n",
    "    \n",
    "#     filename = f'waterlevel_missing_{location}.csv'\n",
    "#     df = pd.read_csv(filename)\n",
    "    \n",
    "#     #### Check size of missing value\n",
    "#     size_of_gap = df['Waterlevel'].isna().sum()\n",
    "\n",
    "#     data = df['Waterlevel'].values.tolist()\n",
    "\n",
    "#     nan_index = None\n",
    "#     for j, value in enumerate(data):\n",
    "#         if value != value:  # Check if the value is NaN\n",
    "#             nan_index = j\n",
    "#             break\n",
    "        \n",
    "#     df_check_before = data[:(3*size_of_gap)+1]\n",
    "#     df_check_after = data[::-1][:3*size_of_gap+1]\n",
    "#     df_miss = data[nan_index:nan_index + size_of_gap]\n",
    "\n",
    "#     last_data = data[:nan_index]\n",
    "#     first_data = data[nan_index+size_of_gap:][::-1]\n",
    "\n",
    "#     # check if missing values is in the first 3 x T data original\n",
    "#     if all(value in df_check_before for value in df_miss):\n",
    "#         print('\\nAll values in df_miss is in the first !!!')\n",
    "\n",
    "#         first_value_combine = transform_to_multivariate(first_data, size_of_gap)\n",
    "                \n",
    "#         df_list = to_df(first_value_combine)\n",
    "        \n",
    "\n",
    "#         X = np.array(df_list.iloc[:, :-1])\n",
    "#         X = scaler.fit_transform(X)\n",
    "        \n",
    "#         y = np.array(df_list.iloc[:, -1])\n",
    "        \n",
    "#         model = model_combine(X)\n",
    "#         model.fit(X, y, epochs=200, batch_size=32, callbacks=[early_stopping], validation_split=0.2)\n",
    "\n",
    "#         data_test = df.values.tolist()[nan_index : nan_index + 2 * size_of_gap][::-1]\n",
    "#         data_test = np.concatenate(data_test).ravel()   \n",
    "\n",
    "#         results_first_combine = []\n",
    "#         for i in range(len(data_test)//2):\n",
    "#             data_first = data_test[i:i+1+size_of_gap]\n",
    "\n",
    "#             data_first[size_of_gap] = model.predict(scaler.transform(np.array(data_first[:size_of_gap]).reshape(1,-1)))\n",
    "\n",
    "#             results_first_combine.append(data_first[size_of_gap])\n",
    "            \n",
    "#         ###################################################################\n",
    "#         print('\\n', 'result of combine only (first):')                        #  \n",
    "#         calculate_metrics_for_combine(results_first_combine,'results_combine (FIRST)')#\n",
    "#         print('\\n')                                                       #\n",
    "#         ###################################################################\n",
    "        \n",
    "#         df = pd.read_csv('Impute_misvalues_hungyen.csv')\n",
    "\n",
    "#         plt.figure(figsize=(10, 6))\n",
    "#         plt.plot(df['Waterlevel'].values.tolist()[nan_index:nan_index+size_of_gap], label='Water Level')\n",
    "#         plt.plot(results_first_combine, label='Predicted Value', linestyle='-')\n",
    "#         plt.title(f'Water Level vs Predicted Value, mising {location}, epochs=200, batch_size=32, callbacks (patience=50), validation_split=0.2')\n",
    "#         plt.xlabel('Index')\n",
    "#         plt.ylabel('Value')\n",
    "#         plt.grid(True)\n",
    "#         plt.legend()\n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig(f'Missing {location}, epochs=200, batch_size=32, callbacks (patience=50), validation_split=0.2.png')\n",
    "#         plt.show()\n",
    "        \n",
    "#     elif all(value in df_check_after for value in df_miss):\n",
    "#         print('\\nAll values in df_miss is in the last !!!')\n",
    "        \n",
    "#         last_value_combine = transform_to_multivariate(last_data, size_of_gap)\n",
    "                \n",
    "#         df_list = to_df(last_value_combine)\n",
    "\n",
    "#         X = np.array(df_list.iloc[:, :-1])\n",
    "#         X = scaler.fit_transform(X)\n",
    "        \n",
    "#         y = np.array(df_list.iloc[:, -1])\n",
    "        \n",
    "#         model = model_combine(X)\n",
    "#         model.fit(X, y, epochs=200, batch_size=32, callbacks=[early_stopping], validation_split=0.2)\n",
    "\n",
    "#         data_test = df.values.tolist()[nan_index - size_of_gap : nan_index + size_of_gap]\n",
    "#         data_test = np.concatenate(data_test).ravel()   \n",
    "\n",
    "#         results_last_combine = []\n",
    "#         for i in range(len(data_test)//2):\n",
    "#             data_last = data_test[i:i+1+size_of_gap]\n",
    "\n",
    "#             data_last[size_of_gap] = model.predict(scaler.transform(np.array(data_last[:size_of_gap]).reshape(1,-1)))\n",
    "\n",
    "#             results_last_combine.append(data_last[size_of_gap])\n",
    "            \n",
    "#         #################################################################\n",
    "#         print('\\n', 'result of combine only (last):')                       #\n",
    "#         calculate_metrics_for_combine(results_last_combine,'results_combine (LAST)')#\n",
    "#         print('\\n')                                                     #\n",
    "#         # #################################################################\n",
    "        \n",
    "#         df = pd.read_csv('Impute_misvalues_hungyen.csv')\n",
    "\n",
    "#         plt.figure(figsize=(10, 6))\n",
    "#         plt.plot(df['Waterlevel'].values.tolist()[nan_index:nan_index+size_of_gap], label='Water Level')\n",
    "#         plt.plot(results_last_combine, label='Predicted Value', linestyle='-')\n",
    "#         plt.title(f'Water Level vs Predicted Value, mising {location}, epochs=200, batch_size=32, callbacks (patience=50), validation_split=0.2')\n",
    "#         plt.xlabel('Index')\n",
    "#         plt.ylabel('Value')\n",
    "#         plt.grid(True)\n",
    "#         plt.legend()\n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig(f'Missing {location}, epochs=200, batch_size=32, callbacks (patience=50), validation_split=0.2.png')\n",
    "#         plt.show()\n",
    "        \n",
    "#     else:\n",
    "#         Da = data[nan_index + size_of_gap:][::-1]\n",
    "\n",
    "#         MDa = transform_to_multivariate(Da, size_of_gap)\n",
    "\n",
    "#         df_MDa = to_df(MDa)\n",
    "\n",
    "#         X_MDa_train = np.array(df_MDa.iloc[:, :-1])\n",
    "#         X_MDa_train = scaler.fit_transform(X_MDa_train)\n",
    "        \n",
    "#         y_MDa_train = np.array(df_MDa.iloc[:, -1])\n",
    "\n",
    "#         model_MDa = model_combine(X_MDa_train)\n",
    "#         model_MDa.fit(X_MDa_train, y_MDa_train, epochs=200, batch_size=32, callbacks=[early_stopping], validation_split=0.2)\n",
    "        \n",
    "#         data_test_after = df.values.tolist()[nan_index:nan_index + 2 * size_of_gap ][::-1]\n",
    "#         data_test_after = np.concatenate(data_test_after).ravel()   \n",
    "\n",
    "#         value_lst_after = []\n",
    "#         for j in range(len(data_test_after)//2):\n",
    "#             data_after = data_test_after[j:j+1+size_of_gap]\n",
    "            \n",
    "#             data_after[size_of_gap] = model_MDa.predict(scaler.transform(np.array(data_after[:size_of_gap]).reshape(1,-1)))\n",
    "            \n",
    "#             value_lst_after.append(data_after[size_of_gap])\n",
    "\n",
    "#         Db = data[:nan_index]\n",
    "\n",
    "#         MDb = transform_to_multivariate(Db, size_of_gap)  \n",
    "\n",
    "#         df_MDb = to_df(MDb)\n",
    "\n",
    "#         X_MDb_train = np.array(df_MDb.iloc[:, :-1])\n",
    "#         X_MDb_train = scaler.fit_transform(X_MDb_train)\n",
    "        \n",
    "#         y_MDb_train = np.array(df_MDb.iloc[:, -1])\n",
    "\n",
    "#         model_MDb = model_combine(X_MDb_train)\n",
    "#         model_MDb.fit(X_MDb_train, y_MDb_train, epochs=200, batch_size=32, callbacks=[early_stopping], validation_split=0.2)\n",
    "\n",
    "#         data_test_before = df.values.tolist()[nan_index - size_of_gap : nan_index + size_of_gap]\n",
    "#         data_test_before = np.concatenate(data_test_before).ravel()   \n",
    "\n",
    "#         value_lst_before = []\n",
    "#         for i in range(len(data_test_before)//2):\n",
    "#             data_before = data_test_before[i:i+1+size_of_gap]\n",
    "            \n",
    "#             data_before[size_of_gap] = model_MDb.predict(scaler.transform(np.array(data_before[:size_of_gap]).reshape(1,-1)))\n",
    "\n",
    "#             value_lst_before.append(data_before[size_of_gap])\n",
    "\n",
    "#         #############################################################################\n",
    "#         print('\\n', 'result of combine only: ')                                         #\n",
    "#         results_combine =  [(x + y)/2 for x,y in zip(value_lst_before, value_lst_after)]#\n",
    "#         calculate_metrics_for_combine(results_combine,'results_combine (MIDDLE)')               #\n",
    "#         print('\\n')                                                                 #\n",
    "#         #############################################################################\n",
    "        \n",
    "#         df = pd.read_csv('Impute_misvalues_hungyen.csv')\n",
    "\n",
    "#         plt.figure(figsize=(10, 6))\n",
    "#         plt.plot(df['Waterlevel'].values.tolist()[nan_index:nan_index+size_of_gap], label='Water Level')\n",
    "#         plt.plot(results_combine, label='Predicted Value', linestyle='-')\n",
    "#         plt.title(f'Water Level vs Predicted Value, mising {location}, epochs=200, batch_size=32, callbacks (patience=50), validation_split=0.2')\n",
    "#         plt.xlabel('Index')\n",
    "#         plt.ylabel('Value')\n",
    "#         plt.grid(True)\n",
    "#         plt.legend()\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()    \n",
    "#         plt.savefig(f'Missing {location}, epochs=200, batch_size=32, callbacks (patience=50), validation_split=0.2.png')\n",
    "    \n",
    "    \n",
    "# current_time = datetime.now()\n",
    "\n",
    "# print('\\n')\n",
    "# print('\\nMean of Similarity combine: ',np.mean(sim_lst_combine))\n",
    "# print('\\nMean of Mean Absoulute Error combine :',np.mean(mae_lst_combine))\n",
    "# print('\\nMean of Root Mean Squared Error combine: ',np.mean(rmse_lst_combine))\n",
    "# print('\\nMean of Fraction of Standard Deviation combine: ',np.mean(fsd_lst_combine)) \n",
    "# print('\\nMean of R-score combine: ', np.mean(r_lst_combine))\n",
    "# print('\\nMean of the Nash Sutcliffe efficiency (NSE): ', np.mean(nse_lst_combine))\n",
    "# print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### epochs=200, batch_size=64, callbacks (patience=50), validation_split=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "# # tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "\n",
    "\n",
    "# for location in range(0,12):\n",
    "    \n",
    "#     filename = f'waterlevel_missing_{location}.csv'\n",
    "#     df = pd.read_csv(filename)\n",
    "    \n",
    "#     #### Check size of missing value\n",
    "#     size_of_gap = df['Waterlevel'].isna().sum()\n",
    "\n",
    "#     data = df['Waterlevel'].values.tolist()\n",
    "\n",
    "#     nan_index = None\n",
    "#     for j, value in enumerate(data):\n",
    "#         if value != value:  # Check if the value is NaN\n",
    "#             nan_index = j\n",
    "#             break\n",
    "        \n",
    "#     df_check_before = data[:(3*size_of_gap)+1]\n",
    "#     df_check_after = data[::-1][:3*size_of_gap+1]\n",
    "#     df_miss = data[nan_index:nan_index + size_of_gap]\n",
    "\n",
    "#     last_data = data[:nan_index]\n",
    "#     first_data = data[nan_index+size_of_gap:][::-1]\n",
    "\n",
    "#     # check if missing values is in the first 3 x T data original\n",
    "#     if all(value in df_check_before for value in df_miss):\n",
    "#         print('\\nAll values in df_miss is in the first !!!')\n",
    "\n",
    "#         first_value_combine = transform_to_multivariate(first_data, size_of_gap)\n",
    "                \n",
    "#         df_list = to_df(first_value_combine)\n",
    "        \n",
    "\n",
    "#         X = np.array(df_list.iloc[:, :-1])\n",
    "#         X = scaler.fit_transform(X)\n",
    "        \n",
    "#         y = np.array(df_list.iloc[:, -1])\n",
    "        \n",
    "#         model = model_combine(X)\n",
    "#         model.fit(X, y, epochs=200, batch_size=64, callbacks=[early_stopping], validation_split=0.2)\n",
    "\n",
    "#         data_test = df.values.tolist()[nan_index : nan_index + 2 * size_of_gap][::-1]\n",
    "#         data_test = np.concatenate(data_test).ravel()   \n",
    "\n",
    "#         results_first_combine = []\n",
    "#         for i in range(len(data_test)//2):\n",
    "#             data_first = data_test[i:i+1+size_of_gap]\n",
    "\n",
    "#             data_first[size_of_gap] = model.predict(scaler.transform(np.array(data_first[:size_of_gap]).reshape(1,-1)))\n",
    "\n",
    "#             results_first_combine.append(data_first[size_of_gap])\n",
    "            \n",
    "#         ###################################################################\n",
    "#         print('\\n', 'result of combine only (first):')                        #  \n",
    "#         calculate_metrics_for_combine(results_first_combine,'results_combine (FIRST)')#\n",
    "#         print('\\n')                                                       #\n",
    "#         ###################################################################\n",
    "        \n",
    "#         df = pd.read_csv('Impute_misvalues_hungyen.csv')\n",
    "\n",
    "#         plt.figure(figsize=(10, 6))\n",
    "#         plt.plot(df['Waterlevel'].values.tolist()[nan_index:nan_index+size_of_gap], label='Water Level')\n",
    "#         plt.plot(results_first_combine, label='Predicted Value', linestyle='-')\n",
    "#         plt.title(f'Water Level vs Predicted Value, mising {location}, epochs=200, batch_size=64, callbacks (patience=50), validation_split=0.2')\n",
    "#         plt.xlabel('Index')\n",
    "#         plt.ylabel('Value')\n",
    "#         plt.grid(True)\n",
    "#         plt.legend()\n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig(f'Missing {location}, epochs=200, batch_size=64, callbacks (patience=50), validation_split=0.2.png')\n",
    "#         plt.show()\n",
    "        \n",
    "#     elif all(value in df_check_after for value in df_miss):\n",
    "#         print('\\nAll values in df_miss is in the last !!!')\n",
    "        \n",
    "#         last_value_combine = transform_to_multivariate(last_data, size_of_gap)\n",
    "                \n",
    "#         df_list = to_df(last_value_combine)\n",
    "\n",
    "#         X = np.array(df_list.iloc[:, :-1])\n",
    "#         X = scaler.fit_transform(X)\n",
    "        \n",
    "#         y = np.array(df_list.iloc[:, -1])\n",
    "        \n",
    "#         model = model_combine(X)\n",
    "#         model.fit(X, y, epochs=200, batch_size=64, callbacks=[early_stopping], validation_split=0.2)\n",
    "\n",
    "#         data_test = df.values.tolist()[nan_index - size_of_gap : nan_index + size_of_gap]\n",
    "#         data_test = np.concatenate(data_test).ravel()   \n",
    "\n",
    "#         results_last_combine = []\n",
    "#         for i in range(len(data_test)//2):\n",
    "#             data_last = data_test[i:i+1+size_of_gap]\n",
    "\n",
    "#             data_last[size_of_gap] = model.predict(scaler.transform(np.array(data_last[:size_of_gap]).reshape(1,-1)))\n",
    "\n",
    "#             results_last_combine.append(data_last[size_of_gap])\n",
    "            \n",
    "#         #################################################################\n",
    "#         print('\\n', 'result of combine only (last):')                       #\n",
    "#         calculate_metrics_for_combine(results_last_combine,'results_combine (LAST)')#\n",
    "#         print('\\n')                                                     #\n",
    "#         # #################################################################\n",
    "        \n",
    "#         df = pd.read_csv('Impute_misvalues_hungyen.csv')\n",
    "\n",
    "#         plt.figure(figsize=(10, 6))\n",
    "#         plt.plot(df['Waterlevel'].values.tolist()[nan_index:nan_index+size_of_gap], label='Water Level')\n",
    "#         plt.plot(results_last_combine, label='Predicted Value', linestyle='-')\n",
    "#         plt.title(f'Water Level vs Predicted Value, mising {location}, epochs=200, batch_size=64, callbacks (patience=50), validation_split=0.2')\n",
    "#         plt.xlabel('Index')\n",
    "#         plt.ylabel('Value')\n",
    "#         plt.grid(True)\n",
    "#         plt.legend()\n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig(f'Missing {location}, epochs=200, batch_size=64, callbacks (patience=50), validation_split=0.2.png')\n",
    "#         plt.show()\n",
    "        \n",
    "#     else:\n",
    "#         Da = data[nan_index + size_of_gap:][::-1]\n",
    "\n",
    "#         MDa = transform_to_multivariate(Da, size_of_gap)\n",
    "\n",
    "#         df_MDa = to_df(MDa)\n",
    "\n",
    "#         X_MDa_train = np.array(df_MDa.iloc[:, :-1])\n",
    "#         X_MDa_train = scaler.fit_transform(X_MDa_train)\n",
    "        \n",
    "#         y_MDa_train = np.array(df_MDa.iloc[:, -1])\n",
    "\n",
    "#         model_MDa = model_combine(X_MDa_train)\n",
    "#         model_MDa.fit(X_MDa_train, y_MDa_train, epochs=200, batch_size=64, callbacks=[early_stopping], validation_split=0.2)\n",
    "        \n",
    "#         data_test_after = df.values.tolist()[nan_index:nan_index + 2 * size_of_gap ][::-1]\n",
    "#         data_test_after = np.concatenate(data_test_after).ravel()   \n",
    "\n",
    "#         value_lst_after = []\n",
    "#         for j in range(len(data_test_after)//2):\n",
    "#             data_after = data_test_after[j:j+1+size_of_gap]\n",
    "            \n",
    "#             data_after[size_of_gap] = model_MDa.predict(scaler.transform(np.array(data_after[:size_of_gap]).reshape(1,-1)))\n",
    "            \n",
    "#             value_lst_after.append(data_after[size_of_gap])\n",
    "\n",
    "#         Db = data[:nan_index]\n",
    "\n",
    "#         MDb = transform_to_multivariate(Db, size_of_gap)  \n",
    "\n",
    "#         df_MDb = to_df(MDb)\n",
    "\n",
    "#         X_MDb_train = np.array(df_MDb.iloc[:, :-1])\n",
    "#         X_MDb_train = scaler.fit_transform(X_MDb_train)\n",
    "        \n",
    "#         y_MDb_train = np.array(df_MDb.iloc[:, -1])\n",
    "\n",
    "#         model_MDb = model_combine(X_MDb_train)\n",
    "#         model_MDb.fit(X_MDb_train, y_MDb_train, epochs=200, batch_size=64, callbacks=[early_stopping], validation_split=0.2)\n",
    "\n",
    "#         data_test_before = df.values.tolist()[nan_index - size_of_gap : nan_index + size_of_gap]\n",
    "#         data_test_before = np.concatenate(data_test_before).ravel()   \n",
    "\n",
    "#         value_lst_before = []\n",
    "#         for i in range(len(data_test_before)//2):\n",
    "#             data_before = data_test_before[i:i+1+size_of_gap]\n",
    "            \n",
    "#             data_before[size_of_gap] = model_MDb.predict(scaler.transform(np.array(data_before[:size_of_gap]).reshape(1,-1)))\n",
    "\n",
    "#             value_lst_before.append(data_before[size_of_gap])\n",
    "\n",
    "#         #############################################################################\n",
    "#         print('\\n', 'result of combine only: ')                                         #\n",
    "#         results_combine =  [(x + y)/2 for x,y in zip(value_lst_before, value_lst_after)]#\n",
    "#         calculate_metrics_for_combine(results_combine,'results_combine (MIDDLE)')               #\n",
    "#         print('\\n')                                                                 #\n",
    "#         #############################################################################\n",
    "        \n",
    "#         df = pd.read_csv('Impute_misvalues_hungyen.csv')\n",
    "\n",
    "#         plt.figure(figsize=(10, 6))\n",
    "#         plt.plot(df['Waterlevel'].values.tolist()[nan_index:nan_index+size_of_gap], label='Water Level')\n",
    "#         plt.plot(results_combine, label='Predicted Value', linestyle='-')\n",
    "#         plt.title(f'Water Level vs Predicted Value, mising {location}, epochs=200, batch_size=64, callbacks (patience=50), validation_split=0.2')\n",
    "#         plt.xlabel('Index')\n",
    "#         plt.ylabel('Value')\n",
    "#         plt.grid(True)\n",
    "#         plt.legend()\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()    \n",
    "#         plt.savefig(f'Missing {location}, epochs=200, batch_size=64, callbacks (patience=50), validation_split=0.2.png')\n",
    "    \n",
    "    \n",
    "# current_time = datetime.now()\n",
    "\n",
    "# print('\\n')\n",
    "# print('\\nMean of Similarity combine: ',np.mean(sim_lst_combine))\n",
    "# print('\\nMean of Mean Absoulute Error combine :',np.mean(mae_lst_combine))\n",
    "# print('\\nMean of Root Mean Squared Error combine: ',np.mean(rmse_lst_combine))\n",
    "# print('\\nMean of Fraction of Standard Deviation combine: ',np.mean(fsd_lst_combine)) \n",
    "# print('\\nMean of R-score combine: ', np.mean(r_lst_combine))\n",
    "# print('\\nMean of the Nash Sutcliffe efficiency (NSE): ', np.mean(nse_lst_combine))\n",
    "# print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### epochs=200, batch_size=128, callbacks (patience=50), validation_split=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "# # tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "\n",
    "\n",
    "# for location in range(0,12):\n",
    "    \n",
    "#     filename = f'waterlevel_missing_{location}.csv'\n",
    "#     df = pd.read_csv(filename)\n",
    "    \n",
    "#     #### Check size of missing value\n",
    "#     size_of_gap = df['Waterlevel'].isna().sum()\n",
    "\n",
    "#     data = df['Waterlevel'].values.tolist()\n",
    "\n",
    "#     nan_index = None\n",
    "#     for j, value in enumerate(data):\n",
    "#         if value != value:  # Check if the value is NaN\n",
    "#             nan_index = j\n",
    "#             break\n",
    "        \n",
    "#     df_check_before = data[:(3*size_of_gap)+1]\n",
    "#     df_check_after = data[::-1][:3*size_of_gap+1]\n",
    "#     df_miss = data[nan_index:nan_index + size_of_gap]\n",
    "\n",
    "#     last_data = data[:nan_index]\n",
    "#     first_data = data[nan_index+size_of_gap:][::-1]\n",
    "\n",
    "#     # check if missing values is in the first 3 x T data original\n",
    "#     if all(value in df_check_before for value in df_miss):\n",
    "#         print('\\nAll values in df_miss is in the first !!!')\n",
    "\n",
    "#         first_value_combine = transform_to_multivariate(first_data, size_of_gap)\n",
    "                \n",
    "#         df_list = to_df(first_value_combine)\n",
    "        \n",
    "\n",
    "#         X = np.array(df_list.iloc[:, :-1])\n",
    "#         X = scaler.fit_transform(X)\n",
    "        \n",
    "#         y = np.array(df_list.iloc[:, -1])\n",
    "        \n",
    "#         model = model_combine(X)\n",
    "#         model.fit(X, y, epochs=200, batch_size=128, callbacks=[early_stopping], validation_split=0.2)\n",
    "\n",
    "#         data_test = df.values.tolist()[nan_index : nan_index + 2 * size_of_gap][::-1]\n",
    "#         data_test = np.concatenate(data_test).ravel()   \n",
    "\n",
    "#         results_first_combine = []\n",
    "#         for i in range(len(data_test)//2):\n",
    "#             data_first = data_test[i:i+1+size_of_gap]\n",
    "\n",
    "#             data_first[size_of_gap] = model.predict(scaler.transform(np.array(data_first[:size_of_gap]).reshape(1,-1)))\n",
    "\n",
    "#             results_first_combine.append(data_first[size_of_gap])\n",
    "            \n",
    "#         ###################################################################\n",
    "#         print('\\n', 'result of combine only (first):')                        #  \n",
    "#         calculate_metrics_for_combine(results_first_combine,'results_combine (FIRST)')#\n",
    "#         print('\\n')                                                       #\n",
    "#         ###################################################################\n",
    "        \n",
    "#         df = pd.read_csv('Impute_misvalues_hungyen.csv')\n",
    "\n",
    "#         plt.figure(figsize=(10, 6))\n",
    "#         plt.plot(df['Waterlevel'].values.tolist()[nan_index:nan_index+size_of_gap], label='Water Level')\n",
    "#         plt.plot(results_first_combine, label='Predicted Value', linestyle='-')\n",
    "#         plt.title(f'Water Level vs Predicted Value, mising {location}, epochs=200, batch_size=128, callbacks (patience=50), validation_split=0.2')\n",
    "#         plt.xlabel('Index')\n",
    "#         plt.ylabel('Value')\n",
    "#         plt.grid(True)\n",
    "#         plt.legend()\n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig(f'Missing {location}, epochs=200, batch_size=128, callbacks (patience=50), validation_split=0.2.png')\n",
    "#         plt.show()\n",
    "        \n",
    "#     elif all(value in df_check_after for value in df_miss):\n",
    "#         print('\\nAll values in df_miss is in the last !!!')\n",
    "        \n",
    "#         last_value_combine = transform_to_multivariate(last_data, size_of_gap)\n",
    "                \n",
    "#         df_list = to_df(last_value_combine)\n",
    "\n",
    "#         X = np.array(df_list.iloc[:, :-1])\n",
    "#         X = scaler.fit_transform(X)\n",
    "        \n",
    "#         y = np.array(df_list.iloc[:, -1])\n",
    "        \n",
    "#         model = model_combine(X)\n",
    "#         model.fit(X, y, epochs=200, batch_size=128, callbacks=[early_stopping], validation_split=0.2)\n",
    "\n",
    "#         data_test = df.values.tolist()[nan_index - size_of_gap : nan_index + size_of_gap]\n",
    "#         data_test = np.concatenate(data_test).ravel()   \n",
    "\n",
    "#         results_last_combine = []\n",
    "#         for i in range(len(data_test)//2):\n",
    "#             data_last = data_test[i:i+1+size_of_gap]\n",
    "\n",
    "#             data_last[size_of_gap] = model.predict(scaler.transform(np.array(data_last[:size_of_gap]).reshape(1,-1)))\n",
    "\n",
    "#             results_last_combine.append(data_last[size_of_gap])\n",
    "            \n",
    "#         #################################################################\n",
    "#         print('\\n', 'result of combine only (last):')                       #\n",
    "#         calculate_metrics_for_combine(results_last_combine,'results_combine (LAST)')#\n",
    "#         print('\\n')                                                     #\n",
    "#         # #################################################################\n",
    "        \n",
    "#         df = pd.read_csv('Impute_misvalues_hungyen.csv')\n",
    "\n",
    "#         plt.figure(figsize=(10, 6))\n",
    "#         plt.plot(df['Waterlevel'].values.tolist()[nan_index:nan_index+size_of_gap], label='Water Level')\n",
    "#         plt.plot(results_last_combine, label='Predicted Value', linestyle='-')\n",
    "#         plt.title(f'Water Level vs Predicted Value, mising {location}, epochs=200, batch_size=128, callbacks (patience=50), validation_split=0.2')\n",
    "#         plt.xlabel('Index')\n",
    "#         plt.ylabel('Value')\n",
    "#         plt.grid(True)\n",
    "#         plt.legend()\n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig(f'Missing {location}, epochs=200, batch_size=128, callbacks (patience=50), validation_split=0.2.png')\n",
    "#         plt.show()\n",
    "        \n",
    "#     else:\n",
    "#         Da = data[nan_index + size_of_gap:][::-1]\n",
    "\n",
    "#         MDa = transform_to_multivariate(Da, size_of_gap)\n",
    "\n",
    "#         df_MDa = to_df(MDa)\n",
    "\n",
    "#         X_MDa_train = np.array(df_MDa.iloc[:, :-1])\n",
    "#         X_MDa_train = scaler.fit_transform(X_MDa_train)\n",
    "        \n",
    "#         y_MDa_train = np.array(df_MDa.iloc[:, -1])\n",
    "\n",
    "#         model_MDa = model_combine(X_MDa_train)\n",
    "#         model_MDa.fit(X_MDa_train, y_MDa_train, epochs=200, batch_size=128, callbacks=[early_stopping], validation_split=0.2)\n",
    "        \n",
    "#         data_test_after = df.values.tolist()[nan_index:nan_index + 2 * size_of_gap ][::-1]\n",
    "#         data_test_after = np.concatenate(data_test_after).ravel()   \n",
    "\n",
    "#         value_lst_after = []\n",
    "#         for j in range(len(data_test_after)//2):\n",
    "#             data_after = data_test_after[j:j+1+size_of_gap]\n",
    "            \n",
    "#             data_after[size_of_gap] = model_MDa.predict(scaler.transform(np.array(data_after[:size_of_gap]).reshape(1,-1)))\n",
    "            \n",
    "#             value_lst_after.append(data_after[size_of_gap])\n",
    "\n",
    "#         Db = data[:nan_index]\n",
    "\n",
    "#         MDb = transform_to_multivariate(Db, size_of_gap)  \n",
    "\n",
    "#         df_MDb = to_df(MDb)\n",
    "\n",
    "#         X_MDb_train = np.array(df_MDb.iloc[:, :-1])\n",
    "#         X_MDb_train = scaler.fit_transform(X_MDb_train)\n",
    "        \n",
    "#         y_MDb_train = np.array(df_MDb.iloc[:, -1])\n",
    "\n",
    "#         model_MDb = model_combine(X_MDb_train)\n",
    "#         model_MDb.fit(X_MDb_train, y_MDb_train, epochs=200, batch_size=128, callbacks=[early_stopping], validation_split=0.2)\n",
    "\n",
    "#         data_test_before = df.values.tolist()[nan_index - size_of_gap : nan_index + size_of_gap]\n",
    "#         data_test_before = np.concatenate(data_test_before).ravel()   \n",
    "\n",
    "#         value_lst_before = []\n",
    "#         for i in range(len(data_test_before)//2):\n",
    "#             data_before = data_test_before[i:i+1+size_of_gap]\n",
    "            \n",
    "#             data_before[size_of_gap] = model_MDb.predict(scaler.transform(np.array(data_before[:size_of_gap]).reshape(1,-1)))\n",
    "\n",
    "#             value_lst_before.append(data_before[size_of_gap])\n",
    "\n",
    "#         #############################################################################\n",
    "#         print('\\n', 'result of combine only: ')                                         #\n",
    "#         results_combine =  [(x + y)/2 for x,y in zip(value_lst_before, value_lst_after)]#\n",
    "#         calculate_metrics_for_combine(results_combine,'results_combine (MIDDLE)')               #\n",
    "#         print('\\n')                                                                 #\n",
    "#         #############################################################################\n",
    "        \n",
    "#         df = pd.read_csv('Impute_misvalues_hungyen.csv')\n",
    "\n",
    "#         plt.figure(figsize=(10, 6))\n",
    "#         plt.plot(df['Waterlevel'].values.tolist()[nan_index:nan_index+size_of_gap], label='Water Level')\n",
    "#         plt.plot(results_combine, label='Predicted Value', linestyle='-')\n",
    "#         plt.title(f'Water Level vs Predicted Value, mising {location}, epochs=200, batch_size=128, callbacks (patience=50), validation_split=0.2')\n",
    "#         plt.xlabel('Index')\n",
    "#         plt.ylabel('Value')\n",
    "#         plt.grid(True)\n",
    "#         plt.legend()\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()    \n",
    "#         plt.savefig(f'Missing {location}, epochs=200, batch_size=128, callbacks (patience=50), validation_split=0.2.png')\n",
    "    \n",
    "    \n",
    "# current_time = datetime.now()\n",
    "\n",
    "# print('\\n')\n",
    "# print('\\nMean of Similarity combine: ',np.mean(sim_lst_combine))\n",
    "# print('\\nMean of Mean Absoulute Error combine :',np.mean(mae_lst_combine))\n",
    "# print('\\nMean of Root Mean Squared Error combine: ',np.mean(rmse_lst_combine))\n",
    "# print('\\nMean of Fraction of Standard Deviation combine: ',np.mean(fsd_lst_combine)) \n",
    "# print('\\nMean of R-score combine: ', np.mean(r_lst_combine))\n",
    "# print('\\nMean of the Nash Sutcliffe efficiency (NSE): ', np.mean(nse_lst_combine))\n",
    "# print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### epochs=200, batch_size=256, callbacks (patience=50), validation_split=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "# # tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "\n",
    "\n",
    "# for location in range(0,12):\n",
    "    \n",
    "#     filename = f'waterlevel_missing_{location}.csv'\n",
    "#     df = pd.read_csv(filename)\n",
    "    \n",
    "#     #### Check size of missing value\n",
    "#     size_of_gap = df['Waterlevel'].isna().sum()\n",
    "\n",
    "#     data = df['Waterlevel'].values.tolist()\n",
    "\n",
    "#     nan_index = None\n",
    "#     for j, value in enumerate(data):\n",
    "#         if value != value:  # Check if the value is NaN\n",
    "#             nan_index = j\n",
    "#             break\n",
    "        \n",
    "#     df_check_before = data[:(3*size_of_gap)+1]\n",
    "#     df_check_after = data[::-1][:3*size_of_gap+1]\n",
    "#     df_miss = data[nan_index:nan_index + size_of_gap]\n",
    "\n",
    "#     last_data = data[:nan_index]\n",
    "#     first_data = data[nan_index+size_of_gap:][::-1]\n",
    "\n",
    "#     # check if missing values is in the first 3 x T data original\n",
    "#     if all(value in df_check_before for value in df_miss):\n",
    "#         print('\\nAll values in df_miss is in the first !!!')\n",
    "\n",
    "#         first_value_combine = transform_to_multivariate(first_data, size_of_gap)\n",
    "                \n",
    "#         df_list = to_df(first_value_combine)\n",
    "        \n",
    "\n",
    "#         X = np.array(df_list.iloc[:, :-1])\n",
    "#         X = scaler.fit_transform(X)\n",
    "        \n",
    "#         y = np.array(df_list.iloc[:, -1])\n",
    "        \n",
    "#         model = model_combine(X)\n",
    "#         model.fit(X, y, epochs=200, batch_size=256, callbacks=[early_stopping], validation_split=0.2)\n",
    "\n",
    "#         data_test = df.values.tolist()[nan_index : nan_index + 2 * size_of_gap][::-1]\n",
    "#         data_test = np.concatenate(data_test).ravel()   \n",
    "\n",
    "#         results_first_combine = []\n",
    "#         for i in range(len(data_test)//2):\n",
    "#             data_first = data_test[i:i+1+size_of_gap]\n",
    "\n",
    "#             data_first[size_of_gap] = model.predict(scaler.transform(np.array(data_first[:size_of_gap]).reshape(1,-1)))\n",
    "\n",
    "#             results_first_combine.append(data_first[size_of_gap])\n",
    "            \n",
    "#         ###################################################################\n",
    "#         print('\\n', 'result of combine only (first):')                        #  \n",
    "#         calculate_metrics_for_combine(results_first_combine,'results_combine (FIRST)')#\n",
    "#         print('\\n')                                                       #\n",
    "#         ###################################################################\n",
    "        \n",
    "#         df = pd.read_csv('Impute_misvalues_hungyen.csv')\n",
    "\n",
    "#         plt.figure(figsize=(10, 6))\n",
    "#         plt.plot(df['Waterlevel'].values.tolist()[nan_index:nan_index+size_of_gap], label='Water Level')\n",
    "#         plt.plot(results_first_combine, label='Predicted Value', linestyle='-')\n",
    "#         plt.title(f'Water Level vs Predicted Value, mising {location}, epochs=200, batch_size=256, callbacks (patience=50), validation_split=0.2')\n",
    "#         plt.xlabel('Index')\n",
    "#         plt.ylabel('Value')\n",
    "#         plt.grid(True)\n",
    "#         plt.legend()\n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig(f'Missing {location}, epochs=200, batch_size=256, callbacks (patience=50), validation_split=0.2.png')\n",
    "#         plt.show()\n",
    "        \n",
    "#     elif all(value in df_check_after for value in df_miss):\n",
    "#         print('\\nAll values in df_miss is in the last !!!')\n",
    "        \n",
    "#         last_value_combine = transform_to_multivariate(last_data, size_of_gap)\n",
    "                \n",
    "#         df_list = to_df(last_value_combine)\n",
    "\n",
    "#         X = np.array(df_list.iloc[:, :-1])\n",
    "#         X = scaler.fit_transform(X)\n",
    "        \n",
    "#         y = np.array(df_list.iloc[:, -1])\n",
    "        \n",
    "#         model = model_combine(X)\n",
    "#         model.fit(X, y, epochs=200, batch_size=256, callbacks=[early_stopping], validation_split=0.2)\n",
    "\n",
    "#         data_test = df.values.tolist()[nan_index - size_of_gap : nan_index + size_of_gap]\n",
    "#         data_test = np.concatenate(data_test).ravel()   \n",
    "\n",
    "#         results_last_combine = []\n",
    "#         for i in range(len(data_test)//2):\n",
    "#             data_last = data_test[i:i+1+size_of_gap]\n",
    "\n",
    "#             data_last[size_of_gap] = model.predict(scaler.transform(np.array(data_last[:size_of_gap]).reshape(1,-1)))\n",
    "\n",
    "#             results_last_combine.append(data_last[size_of_gap])\n",
    "            \n",
    "#         #################################################################\n",
    "#         print('\\n', 'result of combine only (last):')                       #\n",
    "#         calculate_metrics_for_combine(results_last_combine,'results_combine (LAST)')#\n",
    "#         print('\\n')                                                     #\n",
    "#         # #################################################################\n",
    "        \n",
    "#         df = pd.read_csv('Impute_misvalues_hungyen.csv')\n",
    "\n",
    "#         plt.figure(figsize=(10, 6))\n",
    "#         plt.plot(df['Waterlevel'].values.tolist()[nan_index:nan_index+size_of_gap], label='Water Level')\n",
    "#         plt.plot(results_last_combine, label='Predicted Value', linestyle='-')\n",
    "#         plt.title(f'Water Level vs Predicted Value, mising {location}, epochs=200, batch_size=256, callbacks (patience=50), validation_split=0.2')\n",
    "#         plt.xlabel('Index')\n",
    "#         plt.ylabel('Value')\n",
    "#         plt.grid(True)\n",
    "#         plt.legend()\n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig(f'Missing {location}, epochs=200, batch_size=256, callbacks (patience=50), validation_split=0.2.png')\n",
    "#         plt.show()\n",
    "        \n",
    "#     else:\n",
    "#         Da = data[nan_index + size_of_gap:][::-1]\n",
    "\n",
    "#         MDa = transform_to_multivariate(Da, size_of_gap)\n",
    "\n",
    "#         df_MDa = to_df(MDa)\n",
    "\n",
    "#         X_MDa_train = np.array(df_MDa.iloc[:, :-1])\n",
    "#         X_MDa_train = scaler.fit_transform(X_MDa_train)\n",
    "        \n",
    "#         y_MDa_train = np.array(df_MDa.iloc[:, -1])\n",
    "\n",
    "#         model_MDa = model_combine(X_MDa_train)\n",
    "#         model_MDa.fit(X_MDa_train, y_MDa_train, epochs=200, batch_size=256, callbacks=[early_stopping], validation_split=0.2)\n",
    "        \n",
    "#         data_test_after = df.values.tolist()[nan_index:nan_index + 2 * size_of_gap ][::-1]\n",
    "#         data_test_after = np.concatenate(data_test_after).ravel()   \n",
    "\n",
    "#         value_lst_after = []\n",
    "#         for j in range(len(data_test_after)//2):\n",
    "#             data_after = data_test_after[j:j+1+size_of_gap]\n",
    "            \n",
    "#             data_after[size_of_gap] = model_MDa.predict(scaler.transform(np.array(data_after[:size_of_gap]).reshape(1,-1)))\n",
    "            \n",
    "#             value_lst_after.append(data_after[size_of_gap])\n",
    "\n",
    "#         Db = data[:nan_index]\n",
    "\n",
    "#         MDb = transform_to_multivariate(Db, size_of_gap)  \n",
    "\n",
    "#         df_MDb = to_df(MDb)\n",
    "\n",
    "#         X_MDb_train = np.array(df_MDb.iloc[:, :-1])\n",
    "#         X_MDb_train = scaler.fit_transform(X_MDb_train)\n",
    "        \n",
    "#         y_MDb_train = np.array(df_MDb.iloc[:, -1])\n",
    "\n",
    "#         model_MDb = model_combine(X_MDb_train)\n",
    "#         model_MDb.fit(X_MDb_train, y_MDb_train, epochs=200, batch_size=256, callbacks=[early_stopping], validation_split=0.2)\n",
    "\n",
    "#         data_test_before = df.values.tolist()[nan_index - size_of_gap : nan_index + size_of_gap]\n",
    "#         data_test_before = np.concatenate(data_test_before).ravel()   \n",
    "\n",
    "#         value_lst_before = []\n",
    "#         for i in range(len(data_test_before)//2):\n",
    "#             data_before = data_test_before[i:i+1+size_of_gap]\n",
    "            \n",
    "#             data_before[size_of_gap] = model_MDb.predict(scaler.transform(np.array(data_before[:size_of_gap]).reshape(1,-1)))\n",
    "\n",
    "#             value_lst_before.append(data_before[size_of_gap])\n",
    "\n",
    "#         #############################################################################\n",
    "#         print('\\n', 'result of combine only: ')                                         #\n",
    "#         results_combine =  [(x + y)/2 for x,y in zip(value_lst_before, value_lst_after)]#\n",
    "#         calculate_metrics_for_combine(results_combine,'results_combine (MIDDLE)')               #\n",
    "#         print('\\n')                                                                 #\n",
    "#         #############################################################################\n",
    "        \n",
    "#         df = pd.read_csv('Impute_misvalues_hungyen.csv')\n",
    "\n",
    "#         plt.figure(figsize=(10, 6))\n",
    "#         plt.plot(df['Waterlevel'].values.tolist()[nan_index:nan_index+size_of_gap], label='Water Level')\n",
    "#         plt.plot(results_combine, label='Predicted Value', linestyle='-')\n",
    "#         plt.title(f'Water Level vs Predicted Value, mising {location}, epochs=200, batch_size=256, callbacks (patience=50), validation_split=0.2')\n",
    "#         plt.xlabel('Index')\n",
    "#         plt.ylabel('Value')\n",
    "#         plt.grid(True)\n",
    "#         plt.legend()\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()    \n",
    "#         plt.savefig(f'Missing {location}, epochs=200, batch_size=256, callbacks (patience=50), validation_split=0.2.png')\n",
    "    \n",
    "    \n",
    "# current_time = datetime.now()\n",
    "\n",
    "# print('\\n')\n",
    "# print('\\nMean of Similarity combine: ',np.mean(sim_lst_combine))\n",
    "# print('\\nMean of Mean Absoulute Error combine :',np.mean(mae_lst_combine))\n",
    "# print('\\nMean of Root Mean Squared Error combine: ',np.mean(rmse_lst_combine))\n",
    "# print('\\nMean of Fraction of Standard Deviation combine: ',np.mean(fsd_lst_combine)) \n",
    "# print('\\nMean of R-score combine: ', np.mean(r_lst_combine))\n",
    "# print('\\nMean of the Nash Sutcliffe efficiency (NSE): ', np.mean(nse_lst_combine))\n",
    "# print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
